{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "industrial-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos paquetes\n",
    "import sys\n",
    "sys.path.append('Src/')\n",
    "from data_modificado import * #hay funciones que estan cambiadas en este script para adaptralas a nuestro dataset\n",
    "from train_2 import * #este hubo que modificar una linea tambien\n",
    "from transfer_learning import * #hubo que modificart lo mismo que en train_2\n",
    "from test_functions import *\n",
    "from layers import *\n",
    "from utils import *\n",
    "from loss import *\n",
    "from metric import *\n",
    "from results import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import itertools as it\n",
    "\n",
    "#funciones\n",
    "def read_df(\n",
    "              metadata_names=['age','Temperature','Precipitation3Days'],\n",
    "              random_state=42,\n",
    "              otu_filename='../Datasets/otu_table_all_80.csv',\n",
    "              metadata_filename='../Datasets/metadata_table_all_80.csv'):\n",
    "    otu = pd.read_csv(otu_filename, index_col=0, header=None).T\n",
    "    #print(otu.head())\n",
    "    otu = otu.set_index('otuids')\n",
    "    otu = otu.astype('int32')\n",
    "    metadata = pd.read_csv(metadata_filename)\n",
    "    #print(metadata.head())\n",
    "    metadata = metadata.set_index('X.SampleID')\n",
    "    metadata.head()\n",
    "    domain = metadata[metadata_names]\n",
    "    #if 'INBREDS' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['INBREDS'], prefix='INBREDS')], axis=1)\n",
    "    #    domain = domain.drop(['INBREDS'], axis=1)\n",
    "    #elif 'Maize_Line' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['Maize_Line'], prefix='Maize_Line')], axis=1)\n",
    "    #    domain = domain.drop(['Maize_Line'], axis=1) \n",
    "    df = pd.concat([otu, domain], axis=1, sort=True, join='outer')\n",
    "    #print(df.head())\n",
    "    #data_microbioma = df[otu.columns].to_numpy(dtype=np.float32)\n",
    "    #data_domain = df[domain.columns].to_numpy(dtype=np.float32)\n",
    "    df_microbioma = df[otu.columns]\n",
    "    df_domain = df[domain.columns]\n",
    "    df_domain.head()\n",
    "    df_microbioma_train, df_microbioma_no_train, df_domain_train, df_domain_no_train = \\\n",
    "        train_test_split(df_microbioma, df_domain, test_size=0.1, random_state=random_state)\n",
    "    # Transfer learning subset\n",
    "    df_microbioma_test, df_microbioma_transfer_learning, df_domain_test, df_domain_transfer_learning = \\\n",
    "        train_test_split(df_microbioma_no_train, df_domain_no_train, test_size=0.1, random_state=random_state)\n",
    "    df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test = \\\n",
    "        train_test_split(df_microbioma_transfer_learning, df_domain_transfer_learning, test_size=0.3, random_state=random_state)\n",
    "    \n",
    "    return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "    #return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "\n",
    "\n",
    "def train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                batch_size, epochs, train_callbacks):\n",
    "    all_models = model_fn()\n",
    "    model, encoder_bioma, encoder_domain, decoder_bioma = all_models\n",
    "    metrics_prefix = None\n",
    "    if encoder_bioma is not None and encoder_domain is not None:\n",
    "        x_train = (m_train, d_train)\n",
    "        y_train = (m_train, m_train, z_train)\n",
    "        x_test = (m_test, d_test)\n",
    "        y_test = (m_test, m_test, z_test)\n",
    "    elif encoder_bioma is not None:\n",
    "        x_train = m_train\n",
    "        y_train = m_train\n",
    "        x_test = m_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'bioma'\n",
    "    elif encoder_domain is not None:\n",
    "        x_train = d_train\n",
    "        y_train = m_train\n",
    "        x_test = d_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'domain'\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(\n",
    "        batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    r = model.fit(train_dataset,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=val_dataset,\n",
    "                  callbacks=train_callbacks,\n",
    "                  verbose=0)\n",
    "    if metrics_prefix is not None:\n",
    "        old_keys = r.history\n",
    "        r.history = {}\n",
    "        for k, v in old_keys.items():\n",
    "            if k == 'loss' or k == 'val_loss':\n",
    "                new_key = k\n",
    "            elif k.startswith('val_'):\n",
    "                new_key = 'val_{}_{}'.format(metrics_prefix, k[4:])\n",
    "            else:\n",
    "                new_key = '{}_{}'.format(metrics_prefix, k)\n",
    "            r.history[new_key] = v\n",
    "    del val_dataset\n",
    "    del train_dataset\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del x_test\n",
    "    del y_test\n",
    "    return r, all_models\n",
    "\n",
    "def train_2(model_fn,\n",
    "          data_microbioma,\n",
    "          data_domain,\n",
    "          latent_space=10,\n",
    "          folds=5,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          learning_rate_scheduler=ExpDecayScheluder(),\n",
    "          random_seed=347,\n",
    "          verbose=0):\n",
    "    data_zeros_latent = np.zeros((data_microbioma.shape[0], latent_space), dtype=data_microbioma.dtype)\n",
    "    results = []\n",
    "    models = []\n",
    "    train_callbacks = [\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=epochs + 1, restore_best_weights=True)]\n",
    "    if verbose >= 0:\n",
    "        train_callbacks += [TqdmCallback(verbose=verbose)]\n",
    "    if learning_rate_scheduler is not None:\n",
    "        train_callbacks += [learning_rate_scheduler.make()]\n",
    "\n",
    "    if folds <= 1:\n",
    "        m_train, m_test = data_microbioma, data_microbioma\n",
    "        d_train, d_test = data_domain, data_domain\n",
    "        z_train, z_test = data_zeros_latent, data_zeros_latent\n",
    "        tf.random.set_seed(random_seed)\n",
    "        r, m = train_kfold(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                           batch_size, epochs, train_callbacks)\n",
    "        results.append(r)\n",
    "        models.append(m)\n",
    "\n",
    "    else: #EL PROBLEMA ESTA AQUI, QUE HACE FALTA UN \n",
    "        kf = KFold(n_splits=folds, random_state=random_seed, shuffle=True)\n",
    "        tf.random.set_seed(random_seed)\n",
    "\n",
    "        for train_index, test_index in kf.split(data_microbioma):\n",
    "            m_train, m_test = data_microbioma[train_index], data_microbioma[test_index]\n",
    "            #print(m_train)\n",
    "            #d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            if data_domain is None:\n",
    "                d_train, d_test = None, None\n",
    "            else:\n",
    "                d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            #print(d_train)\n",
    "            #Esto de hacer el if else ha funcionado, pero no se si hace lo que debe bien\n",
    "            z_train, z_test = data_zeros_latent[train_index], data_zeros_latent[test_index]\n",
    "            r, m = train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                               batch_size, epochs, train_callbacks)\n",
    "            results.append(r)\n",
    "            models.append(m)\n",
    "    return results, models\n",
    "\n",
    "def perform_experiment_2_mod(cv_folds, epochs, batch_size, learning_rate, optimizer,\n",
    "                       learning_rate_scheduler, input_transform, output_transform,\n",
    "                       reconstruction_loss, latent_space, layers,\n",
    "                       activation, activation_latent,\n",
    "                       data_microbioma_train, data_domain_train,\n",
    "                       show_results=True, device='/CPU:0'): #Show results cambiado de False  aTrue\n",
    "    if input_transform is not None:\n",
    "        input_transform = input_transform()\n",
    "    #----------    \n",
    "    if output_transform is not None:\n",
    "        output_transform = output_transform()\n",
    "    #----------      \n",
    "    if reconstruction_loss.__class__.__name__ == 'MakeLoss':\n",
    "        reconstruction_loss = reconstruction_loss.make()\n",
    "    else:\n",
    "        reconstruction_loss = reconstruction_loss()\n",
    "    domain_layers = [l // 16 for l in layers] ####que es esto???? Esto es para las capas del domain\n",
    "    #print(domain_layers)\n",
    "    bioma_autoencoder = \" -> \".join([\"b\"] +\n",
    "                                    [str(l) for l in layers] +\n",
    "                                    [str(latent_space)] +\n",
    "                                    [str(l) for l in reversed(layers)] +\n",
    "                                    [\"b\"])\n",
    "    #---------- \n",
    "    #esto solo se utiliza para el texto, es irrelevante para nuestro error\n",
    "    if data_domain_train is not None:\n",
    "        domain_autoencoder = \" -> \".join([\"d\"] +\n",
    "                                     [str(l) for l in domain_layers] +\n",
    "                                     [str(latent_space)] +\n",
    "                                     [str(l) for l in reversed(layers)] +\n",
    "                                     [\"b\"])\n",
    "        \n",
    "    else: \n",
    "        domain_autoencoder = \" \"\n",
    "    #---------- \n",
    "    #donde se usa domain autoencoder?\n",
    "    in_transform_name = input_transform.__class__.__name__ if input_transform else \"none\"\n",
    "    out_transform_name = output_transform.__class__.__name__ if output_transform else \"none\"\n",
    "    lr_scheduler_text = learning_rate_scheduler[\n",
    "        1] if learning_rate_scheduler is not None else \"none\"\n",
    "    lr_text = learning_rate if learning_rate_scheduler is not None else \"constant = {}\".format(\n",
    "        learning_rate)\n",
    "    learning_rate_scheduler = learning_rate_scheduler[\n",
    "        0] if learning_rate_scheduler is not None else None\n",
    "    optimizer = optimizer(learning_rate=learning_rate)\n",
    "    #---------- \n",
    "    experiment_parameters = [\n",
    "        (\"Input transform\", in_transform_name),\n",
    "        (\"Output transform\", out_transform_name),\n",
    "        (\"Reconstruction Loss\", reconstruction_loss.__class__.__name__),\n",
    "        (\"Latent Space\", latent_space),\n",
    "        (\"Bioma Autoencoder\", bioma_autoencoder),\n",
    "        (\"Domain Autoencoder\", domain_autoencoder),\n",
    "        (\"Activation Encoder\", activation),\n",
    "        (\"Activation Decoder\", activation),\n",
    "        (\"Activation Latent\", activation_latent),\n",
    "        (\"CV folds\", cv_folds),\n",
    "        (\"Epochs\", epochs),\n",
    "        (\"Batch Size\", batch_size),\n",
    "        (\"Learning Rate Scheduler\", lr_scheduler_text),\n",
    "        (\"Learning Rate\", lr_text),\n",
    "        (\"Optimizer\", optimizer.__class__.__name__),\n",
    "    ]\n",
    "    #----------  \n",
    "    if show_results:\n",
    "        md_text = \"\"\n",
    "        md_text += \"| Parameter             | Value         |\\n\"\n",
    "        md_text += \"|:----------------------|:--------------|\\n\"\n",
    "        for n, v in experiment_parameters:\n",
    "            md_text += \"| {} | {} |\\n\".format(n, v)\n",
    "\n",
    "        display(Markdown(md_text))\n",
    "    #------------\n",
    "    def create_model(print_data=False):\n",
    "        bioma_shape=data_microbioma_train.shape[1]\n",
    "        \n",
    "        if data_domain_train is not None:\n",
    "            domain_shape=data_domain_train.shape[1]\n",
    "            #print(\"data_domain_train!=None\")\n",
    "        else:\n",
    "            domain_shape=None\n",
    "            #print(\"data_domain_train==None\")\n",
    "        models = autoencoder(bioma_shape=bioma_shape,\n",
    "                             #bioma_shape=717,\n",
    "                             domain_shape=domain_shape,\n",
    "                             output_shape=bioma_shape,\n",
    "                             #output_shape=717,\n",
    "                             latent_space=latent_space,\n",
    "                             bioma_layers=layers, #Esto es lo de [512,316]\n",
    "                             domain_layers=domain_layers, #Esto son cada una de las layers divididas por 16\n",
    "                             input_transform=input_transform,\n",
    "                             output_transform=output_transform,\n",
    "                             activation_function_encoder=activation,\n",
    "                             activation_function_decoder=activation,\n",
    "                             activation_function_latent=activation_latent)\n",
    "        #Entiendo analizando lo demas que aqui NO esta el error\n",
    "        #la funcion autoencoder esta en model.py (es la unica funcion en ese script)\n",
    "        \n",
    "        model, encoder_bioma, encoder_domain, decoder_bioma = models\n",
    "\n",
    "        if print_data:\n",
    "            plot_models(model, encoder_bioma, encoder_domain, decoder_bioma)\n",
    "        compile_train(model,\n",
    "                      encoder_bioma=encoder_bioma,\n",
    "                      encoder_domain=encoder_domain,\n",
    "                      reconstruction_error=reconstruction_loss,\n",
    "                      encoded_comparison_error=losses.MeanAbsoluteError(),\n",
    "                      metrics=get_experiment_metrics(input_transform, output_transform),\n",
    "                      optimizer=optimizer)\n",
    "        \n",
    "        #print(\"He acabado create_model :)\")\n",
    "        return model, encoder_bioma, encoder_domain, decoder_bioma\n",
    "    #-----------\n",
    "    create_model(print_data=False)\n",
    "    #-----------\n",
    "    #Esta en esta seccion el problema, en train_2\n",
    "    #print(data_domain_train)\n",
    "    #print(latent_space)\n",
    "    with tf.device(device):\n",
    "        results, models = train_2(create_model,\n",
    "                                data_microbioma_train,\n",
    "                                data_domain_train,\n",
    "                                latent_space=latent_space,\n",
    "                                folds=cv_folds,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                learning_rate_scheduler=learning_rate_scheduler,\n",
    "                                verbose=-1)\n",
    "    #----------\n",
    "    validation_results = print_results(results, show_results=show_results)\n",
    "    if show_results:\n",
    "        display(Markdown(\"*************\"))\n",
    "\n",
    "    return experiment_parameters + validation_results, models, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-turner",
   "metadata": {},
   "source": [
    "__params:__\n",
    " - \"activat_func\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"activ_ouput\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"learning_rate\":[0.01,0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "backed-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "#nombres_metadatos = [\"KCAL\",\"PROT\",\"TFAT\",\"CARB\",\"MOIS\",\"ALC\",\"CAFF\",\"THEO\",\"SUGR\",\"FIBE\",\"CALC\",\"IRON\",\"MAGN\",\"PHOS\",\"POTA\",\"SODI\",\"ZINC\",\"COPP\",\"SELE\",\"VC\",\"VB1\",\"VB2\",\"NIAC\",\"VB6\",\"FOLA\",\"VB12\",\"VARA\",\"RET\",\"BCAR\",\"ACAR\",\"CRYP\",\"LYCO\",\"LZ\",\"ATOC\",\"VK\",\"CHOLE\",\"SFAT\",\"MFAT\",\"PFAT\",\"VITD\",\"CHOLN\"]\n",
    "nombres_metadatos = ['F_TOTAL', 'V_TOTAL', 'V_REDOR_TOTAL', 'V_STARCHY_TOTAL', 'V_LEGUMES', 'G_TOTAL', 'PF_TOTAL', 'PF_MEAT', 'PF_CUREDMEAT', 'PF_ORGAN', 'PF_POULT',\\\n",
    "\t\t\t\t\t'PF_SEAFD_HI', 'PF_SEAFD_LOW', 'PF_EGGS', 'PF_SOY', 'PF_NUTSDS', 'D_TOTAL', 'D_YOGURT', 'OILS', 'SOLID_FATS', 'ADD_SUGARS', 'A_DRINKS']\n",
    "df_microbioma_train, df_microbioma_test, _, _, \\\n",
    "df_domain_train, df_domain_test, _, _, otu_columns, domain_columns = read_df(metadata_names=nombres_metadatos,otu_filename='resultados_ana/datos_otus_metadatos_especies/otu_table_especies_80.csv',metadata_filename='datos-remoto/comidas/metadatos_comidas.csv')\n",
    "\n",
    "data_microbioma_train = df_microbioma_train.to_numpy(dtype=np.float32)\n",
    "data_microbioma_test = df_microbioma_test.to_numpy(dtype=np.float32)\n",
    "data_domain_train = df_domain_train.to_numpy(dtype=np.float32)\n",
    "data_domain_test = df_domain_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "#Preparamos las combinaciones pertinentes (5 mejores)\n",
    "combinations = [[100,64,0.001,optimizers.Adam,15,[512,256],\"tanh\",\"tanh\"],\\\n",
    "                [100,64,0.001,optimizers.Adam,15,[512,256,128],\"tanh\",\"relu\"],\\\n",
    "                [100,96,0.001,optimizers.Adam,15,[512,256],\"tanh\",\"tanh\"],\\\n",
    "                [100,96,0.01,optimizers.Adam,15,[512,256],\"relu\",\"tanh\"],\\\n",
    "                [100,64,0.01,optimizers.Adam,10,[512,256],\"relu\",\"tanh\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-frame",
   "metadata": {},
   "source": [
    "### 0.001, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "animal-gothic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4805366098880768 | 0.4805366098880768 | 0.4805366098880768 |\n",
       "| pearson_corr | 0.625096321105957 | 0.625096321105957 | 0.625096321105957 |\n",
       "| jensen_shannon_divergence | 1.2776437997817993 | 1.2776437997817993 | 1.2776437997817993 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4775315225124359 | 0.4775315225124359 | 0.4775315225124359 |\n",
       "| pearson_corr | 0.5913827419281006 | 0.5913827419281006 | 0.5913827419281006 |\n",
       "| jensen_shannon_divergence | 1.4342634677886963 | 1.4342634677886963 | 1.4342634677886963 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45319750905036926 | 0.45319750905036926 | 0.45319750905036926 |\n",
       "| pearson_corr | 0.6471083760261536 | 0.6471083760261536 | 0.6471083760261536 |\n",
       "| jensen_shannon_divergence | 1.1977444887161255 | 1.1977444887161255 | 1.1977444887161255 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5001455545425415 | 0.5001455545425415 | 0.5001455545425415 |\n",
       "| pearson_corr | 0.561398446559906 | 0.561398446559906 | 0.561398446559906 |\n",
       "| jensen_shannon_divergence | 1.5791258811950684 | 1.5791258811950684 | 1.5791258811950684 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4478285014629364 | 0.4478285014629364 | 0.4478285014629364 |\n",
       "| pearson_corr | 0.638617753982544 | 0.638617753982544 | 0.638617753982544 |\n",
       "| jensen_shannon_divergence | 1.333242416381836 | 1.333242416381836 | 1.333242416381836 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation=None)(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-anatomy",
   "metadata": {},
   "source": [
    "### 0.001, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "auburn-castle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6001875996589661 | 0.6001875996589661 | 0.6001875996589661 |\n",
       "| pearson_corr | 0.5634642839431763 | 0.5634642839431763 | 0.5634642839431763 |\n",
       "| jensen_shannon_divergence | 1.6034209728240967 | 1.6034209728240967 | 1.6034209728240967 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6327547430992126 | 0.6327547430992126 | 0.6327547430992126 |\n",
       "| pearson_corr | 0.490217000246048 | 0.490217000246048 | 0.490217000246048 |\n",
       "| jensen_shannon_divergence | 1.8718141317367554 | 1.8718141317367554 | 1.8718141317367554 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.624006986618042 | 0.624006986618042 | 0.624006986618042 |\n",
       "| pearson_corr | 0.5588113069534302 | 0.5588113069534302 | 0.5588113069534302 |\n",
       "| jensen_shannon_divergence | 1.7412059307098389 | 1.7412059307098389 | 1.7412059307098389 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5906188488006592 | 0.5906188488006592 | 0.5906188488006592 |\n",
       "| pearson_corr | 0.4771423041820526 | 0.4771423041820526 | 0.4771423041820526 |\n",
       "| jensen_shannon_divergence | 1.763100504875183 | 1.763100504875183 | 1.763100504875183 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4927397072315216 | 0.4927397072315216 | 0.4927397072315216 |\n",
       "| pearson_corr | 0.6220669746398926 | 0.6220669746398926 | 0.6220669746398926 |\n",
       "| jensen_shannon_divergence | 1.363389253616333 | 1.363389253616333 | 1.363389253616333 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-cinema",
   "metadata": {},
   "source": [
    "### 0.001, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extra-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5307955741882324 | 0.5307955741882324 | 0.5307955741882324 |\n",
       "| pearson_corr | 0.5753774642944336 | 0.5753774642944336 | 0.5753774642944336 |\n",
       "| jensen_shannon_divergence | 1.3179583549499512 | 1.3179583549499512 | 1.3179583549499512 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4673141837120056 | 0.4673141837120056 | 0.4673141837120056 |\n",
       "| pearson_corr | 0.6193957328796387 | 0.6193957328796387 | 0.6193957328796387 |\n",
       "| jensen_shannon_divergence | 1.2525559663772583 | 1.2525559663772583 | 1.2525559663772583 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5624269843101501 | 0.5624269843101501 | 0.5624269843101501 |\n",
       "| pearson_corr | 0.5961362719535828 | 0.5961362719535828 | 0.5961362719535828 |\n",
       "| jensen_shannon_divergence | 1.4240610599517822 | 1.4240610599517822 | 1.4240610599517822 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.562671959400177 | 0.562671959400177 | 0.562671959400177 |\n",
       "| pearson_corr | 0.5628306865692139 | 0.5628306865692139 | 0.5628306865692139 |\n",
       "| jensen_shannon_divergence | 1.5894299745559692 | 1.5894299745559692 | 1.5894299745559692 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47529488801956177 | 0.47529488801956177 | 0.47529488801956177 |\n",
       "| pearson_corr | 0.6253515481948853 | 0.6253515481948853 | 0.6253515481948853 |\n",
       "| jensen_shannon_divergence | 1.3152778148651123 | 1.3152778148651123 | 1.3152778148651123 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-cologne",
   "metadata": {},
   "source": [
    "### 0.001, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "relevant-working",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.538262665271759 | 0.538262665271759 | 0.538262665271759 |\n",
       "| pearson_corr | 0.5426293015480042 | 0.5426293015480042 | 0.5426293015480042 |\n",
       "| jensen_shannon_divergence | 1.3882941007614136 | 1.3882941007614136 | 1.3882941007614136 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4791297912597656 | 0.4791297912597656 | 0.4791297912597656 |\n",
       "| pearson_corr | 0.5950087904930115 | 0.5950087904930115 | 0.5950087904930115 |\n",
       "| jensen_shannon_divergence | 1.398192048072815 | 1.398192048072815 | 1.398192048072815 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5603542327880859 | 0.5603542327880859 | 0.5603542327880859 |\n",
       "| pearson_corr | 0.5530155897140503 | 0.5530155897140503 | 0.5530155897140503 |\n",
       "| jensen_shannon_divergence | 1.4183884859085083 | 1.4183884859085083 | 1.4183884859085083 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.563652753829956 | 0.563652753829956 | 0.563652753829956 |\n",
       "| pearson_corr | 0.5458134412765503 | 0.5458134412765503 | 0.5458134412765503 |\n",
       "| jensen_shannon_divergence | 1.6148251295089722 | 1.6148251295089722 | 1.6148251295089722 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5108902454376221 | 0.5108902454376221 | 0.5108902454376221 |\n",
       "| pearson_corr | 0.6027514934539795 | 0.6027514934539795 | 0.6027514934539795 |\n",
       "| jensen_shannon_divergence | 1.4575625658035278 | 1.4575625658035278 | 1.4575625658035278 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-password",
   "metadata": {},
   "source": [
    "### 0.001, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "atomic-notice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4675060510635376 | 0.4675060510635376 | 0.4675060510635376 |\n",
       "| pearson_corr | 0.6249818801879883 | 0.6249818801879883 | 0.6249818801879883 |\n",
       "| jensen_shannon_divergence | 1.184651494026184 | 1.184651494026184 | 1.184651494026184 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4801386892795563 | 0.4801386892795563 | 0.4801386892795563 |\n",
       "| pearson_corr | 0.6170819401741028 | 0.6170819401741028 | 0.6170819401741028 |\n",
       "| jensen_shannon_divergence | 1.2843995094299316 | 1.2843995094299316 | 1.2843995094299316 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46985113620758057 | 0.46985113620758057 | 0.46985113620758057 |\n",
       "| pearson_corr | 0.6318854689598083 | 0.6318854689598083 | 0.6318854689598083 |\n",
       "| jensen_shannon_divergence | 1.1879138946533203 | 1.1879138946533203 | 1.1879138946533203 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5212920308113098 | 0.5212920308113098 | 0.5212920308113098 |\n",
       "| pearson_corr | 0.5983790755271912 | 0.5983790755271912 | 0.5983790755271912 |\n",
       "| jensen_shannon_divergence | 1.4273326396942139 | 1.4273326396942139 | 1.4273326396942139 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4947567880153656 | 0.4947567880153656 | 0.4947567880153656 |\n",
       "| pearson_corr | 0.6046473979949951 | 0.6046473979949951 | 0.6046473979949951 |\n",
       "| jensen_shannon_divergence | 1.3972156047821045 | 1.3972156047821045 | 1.3972156047821045 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "  \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-noise",
   "metadata": {},
   "source": [
    "### 0.001, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gorgeous-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5873197913169861 | 0.5873197913169861 | 0.5873197913169861 |\n",
       "| pearson_corr | 0.569516658782959 | 0.569516658782959 | 0.569516658782959 |\n",
       "| jensen_shannon_divergence | 1.5207303762435913 | 1.5207303762435913 | 1.5207303762435913 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6445517539978027 | 0.6445517539978027 | 0.6445517539978027 |\n",
       "| pearson_corr | 0.4979643225669861 | 0.4979643225669861 | 0.4979643225669861 |\n",
       "| jensen_shannon_divergence | 1.8684132099151611 | 1.8684132099151611 | 1.8684132099151611 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6173191666603088 | 0.6173191666603088 | 0.6173191666603088 |\n",
       "| pearson_corr | 0.5581132769584656 | 0.5581132769584656 | 0.5581132769584656 |\n",
       "| jensen_shannon_divergence | 1.6747530698776245 | 1.6747530698776245 | 1.6747530698776245 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5904490947723389 | 0.5904490947723389 | 0.5904490947723389 |\n",
       "| pearson_corr | 0.48134496808052063 | 0.48134496808052063 | 0.48134496808052063 |\n",
       "| jensen_shannon_divergence | 1.7280449867248535 | 1.7280449867248535 | 1.7280449867248535 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49683842062950134 | 0.49683842062950134 | 0.49683842062950134 |\n",
       "| pearson_corr | 0.616696834564209 | 0.616696834564209 | 0.616696834564209 |\n",
       "| jensen_shannon_divergence | 1.4218618869781494 | 1.4218618869781494 | 1.4218618869781494 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-ethiopia",
   "metadata": {},
   "source": [
    "### 0.001, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "statistical-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5424087047576904 | 0.5424087047576904 | 0.5424087047576904 |\n",
       "| pearson_corr | 0.5868561267852783 | 0.5868561267852783 | 0.5868561267852783 |\n",
       "| jensen_shannon_divergence | 1.3533575534820557 | 1.3533575534820557 | 1.3533575534820557 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47496944665908813 | 0.47496944665908813 | 0.47496944665908813 |\n",
       "| pearson_corr | 0.626322865486145 | 0.626322865486145 | 0.626322865486145 |\n",
       "| jensen_shannon_divergence | 1.2816084623336792 | 1.2816084623336792 | 1.2816084623336792 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.567302942276001 | 0.567302942276001 | 0.567302942276001 |\n",
       "| pearson_corr | 0.5836495757102966 | 0.5836495757102966 | 0.5836495757102966 |\n",
       "| jensen_shannon_divergence | 1.4511535167694092 | 1.4511535167694092 | 1.4511535167694092 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5908095836639404 | 0.5908095836639404 | 0.5908095836639404 |\n",
       "| pearson_corr | 0.4991908371448517 | 0.4991908371448517 | 0.4991908371448517 |\n",
       "| jensen_shannon_divergence | 1.6940948963165283 | 1.6940948963165283 | 1.6940948963165283 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.495566725730896 | 0.495566725730896 | 0.495566725730896 |\n",
       "| pearson_corr | 0.6180101037025452 | 0.6180101037025452 | 0.6180101037025452 |\n",
       "| jensen_shannon_divergence | 1.41998291015625 | 1.41998291015625 | 1.41998291015625 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-petite",
   "metadata": {},
   "source": [
    "### 0.001, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "domestic-simon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5407907962799072 | 0.5407907962799072 | 0.5407907962799072 |\n",
       "| pearson_corr | 0.5861192941665649 | 0.5861192941665649 | 0.5861192941665649 |\n",
       "| jensen_shannon_divergence | 1.33469557762146 | 1.33469557762146 | 1.33469557762146 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4680374562740326 | 0.4680374562740326 | 0.4680374562740326 |\n",
       "| pearson_corr | 0.6427944898605347 | 0.6427944898605347 | 0.6427944898605347 |\n",
       "| jensen_shannon_divergence | 1.2594773769378662 | 1.2594773769378662 | 1.2594773769378662 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5613124370574951 | 0.5613124370574951 | 0.5613124370574951 |\n",
       "| pearson_corr | 0.5853227972984314 | 0.5853227972984314 | 0.5853227972984314 |\n",
       "| jensen_shannon_divergence | 1.4094973802566528 | 1.4094973802566528 | 1.4094973802566528 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5861326456069946 | 0.5861326456069946 | 0.5861326456069946 |\n",
       "| pearson_corr | 0.5073991417884827 | 0.5073991417884827 | 0.5073991417884827 |\n",
       "| jensen_shannon_divergence | 1.676128625869751 | 1.676128625869751 | 1.676128625869751 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5092359185218811 | 0.5092359185218811 | 0.5092359185218811 |\n",
       "| pearson_corr | 0.5886557698249817 | 0.5886557698249817 | 0.5886557698249817 |\n",
       "| jensen_shannon_divergence | 1.4987353086471558 | 1.4987353086471558 | 1.4987353086471558 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-antique",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unlike-powder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45377710461616516 | 0.45377710461616516 | 0.45377710461616516 |\n",
       "| pearson_corr | 0.6438016891479492 | 0.6438016891479492 | 0.6438016891479492 |\n",
       "| jensen_shannon_divergence | 1.1479465961456299 | 1.1479465961456299 | 1.1479465961456299 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45613333582878113 | 0.45613333582878113 | 0.45613333582878113 |\n",
       "| pearson_corr | 0.6516469120979309 | 0.6516469120979309 | 0.6516469120979309 |\n",
       "| jensen_shannon_divergence | 1.204972505569458 | 1.204972505569458 | 1.204972505569458 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.44163909554481506 | 0.44163909554481506 | 0.44163909554481506 |\n",
       "| pearson_corr | 0.6648494005203247 | 0.6648494005203247 | 0.6648494005203247 |\n",
       "| jensen_shannon_divergence | 1.1010031700134277 | 1.1010031700134277 | 1.1010031700134277 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4835057854652405 | 0.4835057854652405 | 0.4835057854652405 |\n",
       "| pearson_corr | 0.6221098899841309 | 0.6221098899841309 | 0.6221098899841309 |\n",
       "| jensen_shannon_divergence | 1.3836482763290405 | 1.3836482763290405 | 1.3836482763290405 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4657197892665863 | 0.4657197892665863 | 0.4657197892665863 |\n",
       "| pearson_corr | 0.6356847882270813 | 0.6356847882270813 | 0.6356847882270813 |\n",
       "| jensen_shannon_divergence | 1.267802119255066 | 1.267802119255066 | 1.267802119255066 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-malaysia",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "crucial-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.596490740776062 | 0.596490740776062 | 0.596490740776062 |\n",
       "| pearson_corr | 0.5569028258323669 | 0.5569028258323669 | 0.5569028258323669 |\n",
       "| jensen_shannon_divergence | 1.5869001150131226 | 1.5869001150131226 | 1.5869001150131226 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6342563033103943 | 0.6342563033103943 | 0.6342563033103943 |\n",
       "| pearson_corr | 0.48862314224243164 | 0.48862314224243164 | 0.48862314224243164 |\n",
       "| jensen_shannon_divergence | 1.8540529012680054 | 1.8540529012680054 | 1.8540529012680054 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.61454176902771 | 0.61454176902771 | 0.61454176902771 |\n",
       "| pearson_corr | 0.5511080026626587 | 0.5511080026626587 | 0.5511080026626587 |\n",
       "| jensen_shannon_divergence | 1.6694118976593018 | 1.6694118976593018 | 1.6694118976593018 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5899628400802612 | 0.5899628400802612 | 0.5899628400802612 |\n",
       "| pearson_corr | 0.4900575876235962 | 0.4900575876235962 | 0.4900575876235962 |\n",
       "| jensen_shannon_divergence | 1.7186386585235596 | 1.7186386585235596 | 1.7186386585235596 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49636921286582947 | 0.49636921286582947 | 0.49636921286582947 |\n",
       "| pearson_corr | 0.6216035485267639 | 0.6216035485267639 | 0.6216035485267639 |\n",
       "| jensen_shannon_divergence | 1.400059461593628 | 1.400059461593628 | 1.400059461593628 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-theory",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "broadband-island",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5414367914199829 | 0.5414367914199829 | 0.5414367914199829 |\n",
       "| pearson_corr | 0.5913442969322205 | 0.5913442969322205 | 0.5913442969322205 |\n",
       "| jensen_shannon_divergence | 1.3468478918075562 | 1.3468478918075562 | 1.3468478918075562 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45950567722320557 | 0.45950567722320557 | 0.45950567722320557 |\n",
       "| pearson_corr | 0.6469155550003052 | 0.6469155550003052 | 0.6469155550003052 |\n",
       "| jensen_shannon_divergence | 1.193283200263977 | 1.193283200263977 | 1.193283200263977 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5644726157188416 | 0.5644726157188416 | 0.5644726157188416 |\n",
       "| pearson_corr | 0.5871661305427551 | 0.5871661305427551 | 0.5871661305427551 |\n",
       "| jensen_shannon_divergence | 1.4332544803619385 | 1.4332544803619385 | 1.4332544803619385 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.593237042427063 | 0.593237042427063 | 0.593237042427063 |\n",
       "| pearson_corr | 0.5024652481079102 | 0.5024652481079102 | 0.5024652481079102 |\n",
       "| jensen_shannon_divergence | 1.693637490272522 | 1.693637490272522 | 1.693637490272522 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4975337088108063 | 0.4975337088108063 | 0.4975337088108063 |\n",
       "| pearson_corr | 0.6138879656791687 | 0.6138879656791687 | 0.6138879656791687 |\n",
       "| jensen_shannon_divergence | 1.4269407987594604 | 1.4269407987594604 | 1.4269407987594604 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-honduras",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "starting-swiss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5409899950027466 | 0.5409899950027466 | 0.5409899950027466 |\n",
       "| pearson_corr | 0.5461926460266113 | 0.5461926460266113 | 0.5461926460266113 |\n",
       "| jensen_shannon_divergence | 1.339079737663269 | 1.339079737663269 | 1.339079737663269 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4901297688484192 | 0.4901297688484192 | 0.4901297688484192 |\n",
       "| pearson_corr | 0.6042608022689819 | 0.6042608022689819 | 0.6042608022689819 |\n",
       "| jensen_shannon_divergence | 1.2799420356750488 | 1.2799420356750488 | 1.2799420356750488 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5560488104820251 | 0.5560488104820251 | 0.5560488104820251 |\n",
       "| pearson_corr | 0.5530715584754944 | 0.5530715584754944 | 0.5530715584754944 |\n",
       "| jensen_shannon_divergence | 1.4138474464416504 | 1.4138474464416504 | 1.4138474464416504 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5829952955245972 | 0.5829952955245972 | 0.5829952955245972 |\n",
       "| pearson_corr | 0.5115159749984741 | 0.5115159749984741 | 0.5115159749984741 |\n",
       "| jensen_shannon_divergence | 1.700368046760559 | 1.700368046760559 | 1.700368046760559 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4951007664203644 | 0.4951007664203644 | 0.4951007664203644 |\n",
       "| pearson_corr | 0.6182547211647034 | 0.6182547211647034 | 0.6182547211647034 |\n",
       "| jensen_shannon_divergence | 1.4243485927581787 | 1.4243485927581787 | 1.4243485927581787 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-victory",
   "metadata": {},
   "source": [
    "### 0.001, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unable-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.450536847114563 | 0.450536847114563 | 0.450536847114563 |\n",
       "| pearson_corr | 0.6518262624740601 | 0.6518262624740601 | 0.6518262624740601 |\n",
       "| jensen_shannon_divergence | 1.1798590421676636 | 1.1798590421676636 | 1.1798590421676636 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46526166796684265 | 0.46526166796684265 | 0.46526166796684265 |\n",
       "| pearson_corr | 0.6264206767082214 | 0.6264206767082214 | 0.6264206767082214 |\n",
       "| jensen_shannon_divergence | 1.2109544277191162 | 1.2109544277191162 | 1.2109544277191162 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4675852954387665 | 0.4675852954387665 | 0.4675852954387665 |\n",
       "| pearson_corr | 0.6178867816925049 | 0.6178867816925049 | 0.6178867816925049 |\n",
       "| jensen_shannon_divergence | 1.217771291732788 | 1.217771291732788 | 1.217771291732788 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4477587044239044 | 0.4477587044239044 | 0.4477587044239044 |\n",
       "| pearson_corr | 0.6619058847427368 | 0.6619058847427368 | 0.6619058847427368 |\n",
       "| jensen_shannon_divergence | 1.3054301738739014 | 1.3054301738739014 | 1.3054301738739014 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.475719690322876 | 0.475719690322876 | 0.475719690322876 |\n",
       "| pearson_corr | 0.614625871181488 | 0.614625871181488 | 0.614625871181488 |\n",
       "| jensen_shannon_divergence | 1.3954768180847168 | 1.3954768180847168 | 1.3954768180847168 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-trailer",
   "metadata": {},
   "source": [
    "### 0.001, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "enabling-telescope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5864970684051514 | 0.5864970684051514 | 0.5864970684051514 |\n",
       "| pearson_corr | 0.5558068752288818 | 0.5558068752288818 | 0.5558068752288818 |\n",
       "| jensen_shannon_divergence | 1.5315430164337158 | 1.5315430164337158 | 1.5315430164337158 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6390293836593628 | 0.6390293836593628 | 0.6390293836593628 |\n",
       "| pearson_corr | 0.49406105279922485 | 0.49406105279922485 | 0.49406105279922485 |\n",
       "| jensen_shannon_divergence | 1.8985143899917603 | 1.8985143899917603 | 1.8985143899917603 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6132935881614685 | 0.6132935881614685 | 0.6132935881614685 |\n",
       "| pearson_corr | 0.547741711139679 | 0.547741711139679 | 0.547741711139679 |\n",
       "| jensen_shannon_divergence | 1.6739767789840698 | 1.6739767789840698 | 1.6739767789840698 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5859909653663635 | 0.5859909653663635 | 0.5859909653663635 |\n",
       "| pearson_corr | 0.48144587874412537 | 0.48144587874412537 | 0.48144587874412537 |\n",
       "| jensen_shannon_divergence | 1.7363733053207397 | 1.7363733053207397 | 1.7363733053207397 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49022528529167175 | 0.49022528529167175 | 0.49022528529167175 |\n",
       "| pearson_corr | 0.6221278309822083 | 0.6221278309822083 | 0.6221278309822083 |\n",
       "| jensen_shannon_divergence | 1.370262622833252 | 1.370262622833252 | 1.370262622833252 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-aberdeen",
   "metadata": {},
   "source": [
    "### 0.001, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "military-seller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5307245254516602 | 0.5307245254516602 | 0.5307245254516602 |\n",
       "| pearson_corr | 0.5653515458106995 | 0.5653515458106995 | 0.5653515458106995 |\n",
       "| jensen_shannon_divergence | 1.331281304359436 | 1.331281304359436 | 1.331281304359436 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46887487173080444 | 0.46887487173080444 | 0.46887487173080444 |\n",
       "| pearson_corr | 0.6078193187713623 | 0.6078193187713623 | 0.6078193187713623 |\n",
       "| jensen_shannon_divergence | 1.2894423007965088 | 1.2894423007965088 | 1.2894423007965088 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5500997304916382 | 0.5500997304916382 | 0.5500997304916382 |\n",
       "| pearson_corr | 0.5712795257568359 | 0.5712795257568359 | 0.5712795257568359 |\n",
       "| jensen_shannon_divergence | 1.3903756141662598 | 1.3903756141662598 | 1.3903756141662598 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5768567323684692 | 0.5768567323684692 | 0.5768567323684692 |\n",
       "| pearson_corr | 0.5316789150238037 | 0.5316789150238037 | 0.5316789150238037 |\n",
       "| jensen_shannon_divergence | 1.6753581762313843 | 1.6753581762313843 | 1.6753581762313843 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4885828495025635 | 0.4885828495025635 | 0.4885828495025635 |\n",
       "| pearson_corr | 0.6236727237701416 | 0.6236727237701416 | 0.6236727237701416 |\n",
       "| jensen_shannon_divergence | 1.3382591009140015 | 1.3382591009140015 | 1.3382591009140015 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-hayes",
   "metadata": {},
   "source": [
    "### 0.001, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wrapped-farming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5424736738204956 | 0.5424736738204956 | 0.5424736738204956 |\n",
       "| pearson_corr | 0.5311582088470459 | 0.5311582088470459 | 0.5311582088470459 |\n",
       "| jensen_shannon_divergence | 1.3858336210250854 | 1.3858336210250854 | 1.3858336210250854 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47212597727775574 | 0.47212597727775574 | 0.47212597727775574 |\n",
       "| pearson_corr | 0.6160202622413635 | 0.6160202622413635 | 0.6160202622413635 |\n",
       "| jensen_shannon_divergence | 1.2930402755737305 | 1.2930402755737305 | 1.2930402755737305 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5703983902931213 | 0.5703983902931213 | 0.5703983902931213 |\n",
       "| pearson_corr | 0.5389944911003113 | 0.5389944911003113 | 0.5389944911003113 |\n",
       "| jensen_shannon_divergence | 1.435006022453308 | 1.435006022453308 | 1.435006022453308 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5976755023002625 | 0.5976755023002625 | 0.5976755023002625 |\n",
       "| pearson_corr | 0.47857406735420227 | 0.47857406735420227 | 0.47857406735420227 |\n",
       "| jensen_shannon_divergence | 1.8306392431259155 | 1.8306392431259155 | 1.8306392431259155 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47949063777923584 | 0.47949063777923584 | 0.47949063777923584 |\n",
       "| pearson_corr | 0.6236826181411743 | 0.6236826181411743 | 0.6236826181411743 |\n",
       "| jensen_shannon_divergence | 1.3180664777755737 | 1.3180664777755737 | 1.3180664777755737 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-shooting",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "### 0.01, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sorted-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47185179591178894 | 0.47185179591178894 | 0.47185179591178894 |\n",
       "| pearson_corr | 0.6154182553291321 | 0.6154182553291321 | 0.6154182553291321 |\n",
       "| jensen_shannon_divergence | 1.217748761177063 | 1.217748761177063 | 1.217748761177063 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48116570711135864 | 0.48116570711135864 | 0.48116570711135864 |\n",
       "| pearson_corr | 0.612680971622467 | 0.612680971622467 | 0.612680971622467 |\n",
       "| jensen_shannon_divergence | 1.2938724756240845 | 1.2938724756240845 | 1.2938724756240845 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4798385500907898 | 0.4798385500907898 | 0.4798385500907898 |\n",
       "| pearson_corr | 0.6304316520690918 | 0.6304316520690918 | 0.6304316520690918 |\n",
       "| jensen_shannon_divergence | 1.2528889179229736 | 1.2528889179229736 | 1.2528889179229736 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5442043542861938 | 0.5442043542861938 | 0.5442043542861938 |\n",
       "| pearson_corr | 0.5791927576065063 | 0.5791927576065063 | 0.5791927576065063 |\n",
       "| jensen_shannon_divergence | 1.5329078435897827 | 1.5329078435897827 | 1.5329078435897827 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4966353476047516 | 0.4966353476047516 | 0.4966353476047516 |\n",
       "| pearson_corr | 0.602621853351593 | 0.602621853351593 | 0.602621853351593 |\n",
       "| jensen_shannon_divergence | 1.41573965549469 | 1.41573965549469 | 1.41573965549469 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-mixer",
   "metadata": {},
   "source": [
    "### 0.01, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "golden-channel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5890794992446899 | 0.5890794992446899 | 0.5890794992446899 |\n",
       "| pearson_corr | 0.5695134997367859 | 0.5695134997367859 | 0.5695134997367859 |\n",
       "| jensen_shannon_divergence | 1.531375765800476 | 1.531375765800476 | 1.531375765800476 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6460238695144653 | 0.6460238695144653 | 0.6460238695144653 |\n",
       "| pearson_corr | 0.49360039830207825 | 0.49360039830207825 | 0.49360039830207825 |\n",
       "| jensen_shannon_divergence | 1.8824310302734375 | 1.8824310302734375 | 1.8824310302734375 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6164935231208801 | 0.6164935231208801 | 0.6164935231208801 |\n",
       "| pearson_corr | 0.5591986179351807 | 0.5591986179351807 | 0.5591986179351807 |\n",
       "| jensen_shannon_divergence | 1.6701602935791016 | 1.6701602935791016 | 1.6701602935791016 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5912092328071594 | 0.5912092328071594 | 0.5912092328071594 |\n",
       "| pearson_corr | 0.4810745120048523 | 0.4810745120048523 | 0.4810745120048523 |\n",
       "| jensen_shannon_divergence | 1.727998971939087 | 1.727998971939087 | 1.727998971939087 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49580878019332886 | 0.49580878019332886 | 0.49580878019332886 |\n",
       "| pearson_corr | 0.614694356918335 | 0.614694356918335 | 0.614694356918335 |\n",
       "| jensen_shannon_divergence | 1.4171581268310547 | 1.4171581268310547 | 1.4171581268310547 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-worst",
   "metadata": {},
   "source": [
    "### 0.01, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "involved-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5422801375389099 | 0.5422801375389099 | 0.5422801375389099 |\n",
       "| pearson_corr | 0.587047278881073 | 0.587047278881073 | 0.587047278881073 |\n",
       "| jensen_shannon_divergence | 1.3503721952438354 | 1.3503721952438354 | 1.3503721952438354 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47708481550216675 | 0.47708481550216675 | 0.47708481550216675 |\n",
       "| pearson_corr | 0.6193339228630066 | 0.6193339228630066 | 0.6193339228630066 |\n",
       "| jensen_shannon_divergence | 1.2904049158096313 | 1.2904049158096313 | 1.2904049158096313 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5680772662162781 | 0.5680772662162781 | 0.5680772662162781 |\n",
       "| pearson_corr | 0.577403724193573 | 0.577403724193573 | 0.577403724193573 |\n",
       "| jensen_shannon_divergence | 1.454149603843689 | 1.454149603843689 | 1.454149603843689 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5933889150619507 | 0.5933889150619507 | 0.5933889150619507 |\n",
       "| pearson_corr | 0.48848485946655273 | 0.48848485946655273 | 0.48848485946655273 |\n",
       "| jensen_shannon_divergence | 1.7127653360366821 | 1.7127653360366821 | 1.7127653360366821 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49638935923576355 | 0.49638935923576355 | 0.49638935923576355 |\n",
       "| pearson_corr | 0.6168349385261536 | 0.6168349385261536 | 0.6168349385261536 |\n",
       "| jensen_shannon_divergence | 1.421234130859375 | 1.421234130859375 | 1.421234130859375 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-husband",
   "metadata": {},
   "source": [
    "### 0.01, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affected-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5539107918739319 | 0.5539107918739319 | 0.5539107918739319 |\n",
       "| pearson_corr | 0.539512038230896 | 0.539512038230896 | 0.539512038230896 |\n",
       "| jensen_shannon_divergence | 1.3771109580993652 | 1.3771109580993652 | 1.3771109580993652 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4804069399833679 | 0.4804069399833679 | 0.4804069399833679 |\n",
       "| pearson_corr | 0.6227656602859497 | 0.6227656602859497 | 0.6227656602859497 |\n",
       "| jensen_shannon_divergence | 1.3032777309417725 | 1.3032777309417725 | 1.3032777309417725 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5789722800254822 | 0.5789722800254822 | 0.5789722800254822 |\n",
       "| pearson_corr | 0.5717782378196716 | 0.5717782378196716 | 0.5717782378196716 |\n",
       "| jensen_shannon_divergence | 1.4909402132034302 | 1.4909402132034302 | 1.4909402132034302 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6472703218460083 | 0.6472703218460083 | 0.6472703218460083 |\n",
       "| pearson_corr | 0.3543688952922821 | 0.3543688952922821 | 0.3543688952922821 |\n",
       "| jensen_shannon_divergence | 2.0513923168182373 | 2.0513923168182373 | 2.0513923168182373 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4927695691585541 | 0.4927695691585541 | 0.4927695691585541 |\n",
       "| pearson_corr | 0.6284472942352295 | 0.6284472942352295 | 0.6284472942352295 |\n",
       "| jensen_shannon_divergence | 1.3787586688995361 | 1.3787586688995361 | 1.3787586688995361 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-badge",
   "metadata": {},
   "source": [
    "### 0.01, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "organic-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.463784396648407 | 0.463784396648407 | 0.463784396648407 |\n",
       "| pearson_corr | 0.6310739517211914 | 0.6310739517211914 | 0.6310739517211914 |\n",
       "| jensen_shannon_divergence | 1.1652400493621826 | 1.1652400493621826 | 1.1652400493621826 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4721786677837372 | 0.4721786677837372 | 0.4721786677837372 |\n",
       "| pearson_corr | 0.6262760162353516 | 0.6262760162353516 | 0.6262760162353516 |\n",
       "| jensen_shannon_divergence | 1.2640461921691895 | 1.2640461921691895 | 1.2640461921691895 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.476775586605072 | 0.476775586605072 | 0.476775586605072 |\n",
       "| pearson_corr | 0.6278623938560486 | 0.6278623938560486 | 0.6278623938560486 |\n",
       "| jensen_shannon_divergence | 1.201675295829773 | 1.201675295829773 | 1.201675295829773 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.537815272808075 | 0.537815272808075 | 0.537815272808075 |\n",
       "| pearson_corr | 0.5780180096626282 | 0.5780180096626282 | 0.5780180096626282 |\n",
       "| jensen_shannon_divergence | 1.5327320098876953 | 1.5327320098876953 | 1.5327320098876953 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49045419692993164 | 0.49045419692993164 | 0.49045419692993164 |\n",
       "| pearson_corr | 0.6118457913398743 | 0.6118457913398743 | 0.6118457913398743 |\n",
       "| jensen_shannon_divergence | 1.372780442237854 | 1.372780442237854 | 1.372780442237854 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-zoning",
   "metadata": {},
   "source": [
    "### 0.01, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "optional-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5884732604026794 | 0.5884732604026794 | 0.5884732604026794 |\n",
       "| pearson_corr | 0.5692443251609802 | 0.5692443251609802 | 0.5692443251609802 |\n",
       "| jensen_shannon_divergence | 1.527906894683838 | 1.527906894683838 | 1.527906894683838 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6394171118736267 | 0.6394171118736267 | 0.6394171118736267 |\n",
       "| pearson_corr | 0.5088158845901489 | 0.5088158845901489 | 0.5088158845901489 |\n",
       "| jensen_shannon_divergence | 1.8363369703292847 | 1.8363369703292847 | 1.8363369703292847 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6166061758995056 | 0.6166061758995056 | 0.6166061758995056 |\n",
       "| pearson_corr | 0.5591663122177124 | 0.5591663122177124 | 0.5591663122177124 |\n",
       "| jensen_shannon_divergence | 1.6708323955535889 | 1.6708323955535889 | 1.6708323955535889 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5911292433738708 | 0.5911292433738708 | 0.5911292433738708 |\n",
       "| pearson_corr | 0.4821214973926544 | 0.4821214973926544 | 0.4821214973926544 |\n",
       "| jensen_shannon_divergence | 1.724567174911499 | 1.724567174911499 | 1.724567174911499 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49522846937179565 | 0.49522846937179565 | 0.49522846937179565 |\n",
       "| pearson_corr | 0.6152331233024597 | 0.6152331233024597 | 0.6152331233024597 |\n",
       "| jensen_shannon_divergence | 1.4066143035888672 | 1.4066143035888672 | 1.4066143035888672 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-aaron",
   "metadata": {},
   "source": [
    "### 0.01, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "parliamentary-score",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5413151383399963 | 0.5413151383399963 | 0.5413151383399963 |\n",
       "| pearson_corr | 0.5919100642204285 | 0.5919100642204285 | 0.5919100642204285 |\n",
       "| jensen_shannon_divergence | 1.3468931913375854 | 1.3468931913375854 | 1.3468931913375854 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47169631719589233 | 0.47169631719589233 | 0.47169631719589233 |\n",
       "| pearson_corr | 0.6243536472320557 | 0.6243536472320557 | 0.6243536472320557 |\n",
       "| jensen_shannon_divergence | 1.2730300426483154 | 1.2730300426483154 | 1.2730300426483154 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5646067261695862 | 0.5646067261695862 | 0.5646067261695862 |\n",
       "| pearson_corr | 0.5872585773468018 | 0.5872585773468018 | 0.5872585773468018 |\n",
       "| jensen_shannon_divergence | 1.4334783554077148 | 1.4334783554077148 | 1.4334783554077148 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5935360789299011 | 0.5935360789299011 | 0.5935360789299011 |\n",
       "| pearson_corr | 0.5019571781158447 | 0.5019571781158447 | 0.5019571781158447 |\n",
       "| jensen_shannon_divergence | 1.6946351528167725 | 1.6946351528167725 | 1.6946351528167725 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49768906831741333 | 0.49768906831741333 | 0.49768906831741333 |\n",
       "| pearson_corr | 0.6135697960853577 | 0.6135697960853577 | 0.6135697960853577 |\n",
       "| jensen_shannon_divergence | 1.427812933921814 | 1.427812933921814 | 1.427812933921814 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-typing",
   "metadata": {},
   "source": [
    "### 0.01, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fifty-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5456269383430481 | 0.5456269383430481 | 0.5456269383430481 |\n",
       "| pearson_corr | 0.5812503695487976 | 0.5812503695487976 | 0.5812503695487976 |\n",
       "| jensen_shannon_divergence | 1.3451234102249146 | 1.3451234102249146 | 1.3451234102249146 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46669766306877136 | 0.46669766306877136 | 0.46669766306877136 |\n",
       "| pearson_corr | 0.6330732703208923 | 0.6330732703208923 | 0.6330732703208923 |\n",
       "| jensen_shannon_divergence | 1.2417933940887451 | 1.2417933940887451 | 1.2417933940887451 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5650300979614258 | 0.5650300979614258 | 0.5650300979614258 |\n",
       "| pearson_corr | 0.576276957988739 | 0.576276957988739 | 0.576276957988739 |\n",
       "| jensen_shannon_divergence | 1.427911639213562 | 1.427911639213562 | 1.427911639213562 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5886719226837158 | 0.5886719226837158 | 0.5886719226837158 |\n",
       "| pearson_corr | 0.5015778541564941 | 0.5015778541564941 | 0.5015778541564941 |\n",
       "| jensen_shannon_divergence | 1.6870161294937134 | 1.6870161294937134 | 1.6870161294937134 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5244675278663635 | 0.5244675278663635 | 0.5244675278663635 |\n",
       "| pearson_corr | 0.5695959329605103 | 0.5695959329605103 | 0.5695959329605103 |\n",
       "| jensen_shannon_divergence | 1.5786405801773071 | 1.5786405801773071 | 1.5786405801773071 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-joining",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "experienced-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46929019689559937 | 0.46929019689559937 | 0.46929019689559937 |\n",
       "| pearson_corr | 0.6296336054801941 | 0.6296336054801941 | 0.6296336054801941 |\n",
       "| jensen_shannon_divergence | 1.2746973037719727 | 1.2746973037719727 | 1.2746973037719727 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4660244286060333 | 0.4660244286060333 | 0.4660244286060333 |\n",
       "| pearson_corr | 0.635574221611023 | 0.635574221611023 | 0.635574221611023 |\n",
       "| jensen_shannon_divergence | 1.2561781406402588 | 1.2561781406402588 | 1.2561781406402588 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4628702402114868 | 0.4628702402114868 | 0.4628702402114868 |\n",
       "| pearson_corr | 0.617818295955658 | 0.617818295955658 | 0.617818295955658 |\n",
       "| jensen_shannon_divergence | 1.2535488605499268 | 1.2535488605499268 | 1.2535488605499268 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47374892234802246 | 0.47374892234802246 | 0.47374892234802246 |\n",
       "| pearson_corr | 0.6404688358306885 | 0.6404688358306885 | 0.6404688358306885 |\n",
       "| jensen_shannon_divergence | 1.3556456565856934 | 1.3556456565856934 | 1.3556456565856934 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4523907005786896 | 0.4523907005786896 | 0.4523907005786896 |\n",
       "| pearson_corr | 0.6476398706436157 | 0.6476398706436157 | 0.6476398706436157 |\n",
       "| jensen_shannon_divergence | 1.2991381883621216 | 1.2991381883621216 | 1.2991381883621216 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-independence",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "potential-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5928510427474976 | 0.5928510427474976 | 0.5928510427474976 |\n",
       "| pearson_corr | 0.5661187767982483 | 0.5661187767982483 | 0.5661187767982483 |\n",
       "| jensen_shannon_divergence | 1.5550379753112793 | 1.5550379753112793 | 1.5550379753112793 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6393083333969116 | 0.6393083333969116 | 0.6393083333969116 |\n",
       "| pearson_corr | 0.5068081617355347 | 0.5068081617355347 | 0.5068081617355347 |\n",
       "| jensen_shannon_divergence | 1.8494629859924316 | 1.8494629859924316 | 1.8494629859924316 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.617134153842926 | 0.617134153842926 | 0.617134153842926 |\n",
       "| pearson_corr | 0.5591524243354797 | 0.5591524243354797 | 0.5591524243354797 |\n",
       "| jensen_shannon_divergence | 1.674019455909729 | 1.674019455909729 | 1.674019455909729 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5923035740852356 | 0.5923035740852356 | 0.5923035740852356 |\n",
       "| pearson_corr | 0.48401114344596863 | 0.48401114344596863 | 0.48401114344596863 |\n",
       "| jensen_shannon_divergence | 1.7242666482925415 | 1.7242666482925415 | 1.7242666482925415 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5001022815704346 | 0.5001022815704346 | 0.5001022815704346 |\n",
       "| pearson_corr | 0.6033743619918823 | 0.6033743619918823 | 0.6033743619918823 |\n",
       "| jensen_shannon_divergence | 1.4290117025375366 | 1.4290117025375366 | 1.4290117025375366 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-worry",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "listed-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5428560376167297 | 0.5428560376167297 | 0.5428560376167297 |\n",
       "| pearson_corr | 0.5865004658699036 | 0.5865004658699036 | 0.5865004658699036 |\n",
       "| jensen_shannon_divergence | 1.352591633796692 | 1.352591633796692 | 1.352591633796692 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48195162415504456 | 0.48195162415504456 | 0.48195162415504456 |\n",
       "| pearson_corr | 0.6206144094467163 | 0.6206144094467163 | 0.6206144094467163 |\n",
       "| jensen_shannon_divergence | 1.2290713787078857 | 1.2290713787078857 | 1.2290713787078857 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5672114491462708 | 0.5672114491462708 | 0.5672114491462708 |\n",
       "| pearson_corr | 0.577519953250885 | 0.577519953250885 | 0.577519953250885 |\n",
       "| jensen_shannon_divergence | 1.4497863054275513 | 1.4497863054275513 | 1.4497863054275513 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5933076739311218 | 0.5933076739311218 | 0.5933076739311218 |\n",
       "| pearson_corr | 0.49069565534591675 | 0.49069565534591675 | 0.49069565534591675 |\n",
       "| jensen_shannon_divergence | 1.708763599395752 | 1.708763599395752 | 1.708763599395752 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.496796190738678 | 0.496796190738678 | 0.496796190738678 |\n",
       "| pearson_corr | 0.6139892935752869 | 0.6139892935752869 | 0.6139892935752869 |\n",
       "| jensen_shannon_divergence | 1.415961742401123 | 1.415961742401123 | 1.415961742401123 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-momentum",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecological-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5543439984321594 | 0.5543439984321594 | 0.5543439984321594 |\n",
       "| pearson_corr | 0.5205754637718201 | 0.5205754637718201 | 0.5205754637718201 |\n",
       "| jensen_shannon_divergence | 1.3660242557525635 | 1.3660242557525635 | 1.3660242557525635 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4828846752643585 | 0.4828846752643585 | 0.4828846752643585 |\n",
       "| pearson_corr | 0.603183925151825 | 0.603183925151825 | 0.603183925151825 |\n",
       "| jensen_shannon_divergence | 1.4078378677368164 | 1.4078378677368164 | 1.4078378677368164 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5744435787200928 | 0.5744435787200928 | 0.5744435787200928 |\n",
       "| pearson_corr | 0.5073321461677551 | 0.5073321461677551 | 0.5073321461677551 |\n",
       "| jensen_shannon_divergence | 1.4680334329605103 | 1.4680334329605103 | 1.4680334329605103 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5830832719802856 | 0.5830832719802856 | 0.5830832719802856 |\n",
       "| pearson_corr | 0.48991090059280396 | 0.48991090059280396 | 0.48991090059280396 |\n",
       "| jensen_shannon_divergence | 1.761276125907898 | 1.761276125907898 | 1.761276125907898 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5000730156898499 | 0.5000730156898499 | 0.5000730156898499 |\n",
       "| pearson_corr | 0.6181396842002869 | 0.6181396842002869 | 0.6181396842002869 |\n",
       "| jensen_shannon_divergence | 1.394651174545288 | 1.394651174545288 | 1.394651174545288 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-thing",
   "metadata": {},
   "source": [
    "### 0.01, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "conventional-preservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4580466151237488 | 0.4580466151237488 | 0.4580466151237488 |\n",
       "| pearson_corr | 0.6450393199920654 | 0.6450393199920654 | 0.6450393199920654 |\n",
       "| jensen_shannon_divergence | 1.1763347387313843 | 1.1763347387313843 | 1.1763347387313843 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4524895250797272 | 0.4524895250797272 | 0.4524895250797272 |\n",
       "| pearson_corr | 0.6494733691215515 | 0.6494733691215515 | 0.6494733691215515 |\n",
       "| jensen_shannon_divergence | 1.2070879936218262 | 1.2070879936218262 | 1.2070879936218262 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45180806517601013 | 0.45180806517601013 | 0.45180806517601013 |\n",
       "| pearson_corr | 0.6538452506065369 | 0.6538452506065369 | 0.6538452506065369 |\n",
       "| jensen_shannon_divergence | 1.1485754251480103 | 1.1485754251480103 | 1.1485754251480103 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5588112473487854 | 0.5588112473487854 | 0.5588112473487854 |\n",
       "| pearson_corr | 0.5562357902526855 | 0.5562357902526855 | 0.5562357902526855 |\n",
       "| jensen_shannon_divergence | 1.6098650693893433 | 1.6098650693893433 | 1.6098650693893433 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5008090138435364 | 0.5008090138435364 | 0.5008090138435364 |\n",
       "| pearson_corr | 0.5950855016708374 | 0.5950855016708374 | 0.5950855016708374 |\n",
       "| jensen_shannon_divergence | 1.433365821838379 | 1.433365821838379 | 1.433365821838379 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-fifth",
   "metadata": {},
   "source": [
    "### 0.01, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "floral-print",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7453762888908386 | 0.7453762888908386 | 0.7453762888908386 |\n",
       "| pearson_corr | 0.24130350351333618 | 0.24130350351333618 | 0.24130350351333618 |\n",
       "| jensen_shannon_divergence | 2.6138551235198975 | 2.6138551235198975 | 2.6138551235198975 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6372242569923401 | 0.6372242569923401 | 0.6372242569923401 |\n",
       "| pearson_corr | 0.5124882459640503 | 0.5124882459640503 | 0.5124882459640503 |\n",
       "| jensen_shannon_divergence | 1.8456066846847534 | 1.8456066846847534 | 1.8456066846847534 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.74514240026474 | 0.74514240026474 | 0.74514240026474 |\n",
       "| pearson_corr | 0.34005752205848694 | 0.34005752205848694 | 0.34005752205848694 |\n",
       "| jensen_shannon_divergence | 3.026334047317505 | 3.026334047317505 | 3.026334047317505 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5945754051208496 | 0.5945754051208496 | 0.5945754051208496 |\n",
       "| pearson_corr | 0.4792262017726898 | 0.4792262017726898 | 0.4792262017726898 |\n",
       "| jensen_shannon_divergence | 1.741890549659729 | 1.741890549659729 | 1.741890549659729 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48866403102874756 | 0.48866403102874756 | 0.48866403102874756 |\n",
       "| pearson_corr | 0.6273646950721741 | 0.6273646950721741 | 0.6273646950721741 |\n",
       "| jensen_shannon_divergence | 1.340421438217163 | 1.340421438217163 | 1.340421438217163 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-headset",
   "metadata": {},
   "source": [
    "### 0.01, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "stopped-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5712888836860657 | 0.5712888836860657 | 0.5712888836860657 |\n",
       "| pearson_corr | 0.44382306933403015 | 0.44382306933403015 | 0.44382306933403015 |\n",
       "| jensen_shannon_divergence | 1.4426497220993042 | 1.4426497220993042 | 1.4426497220993042 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4597261846065521 | 0.4597261846065521 | 0.4597261846065521 |\n",
       "| pearson_corr | 0.6482595205307007 | 0.6482595205307007 | 0.6482595205307007 |\n",
       "| jensen_shannon_divergence | 1.2516006231307983 | 1.2516006231307983 | 1.2516006231307983 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5919493436813354 | 0.5919493436813354 | 0.5919493436813354 |\n",
       "| pearson_corr | 0.42765146493911743 | 0.42765146493911743 | 0.42765146493911743 |\n",
       "| jensen_shannon_divergence | 1.5325006246566772 | 1.5325006246566772 | 1.5325006246566772 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5898919105529785 | 0.5898919105529785 | 0.5898919105529785 |\n",
       "| pearson_corr | 0.5116769671440125 | 0.5116769671440125 | 0.5116769671440125 |\n",
       "| jensen_shannon_divergence | 1.7275757789611816 | 1.7275757789611816 | 1.7275757789611816 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.485655277967453 | 0.485655277967453 | 0.485655277967453 |\n",
       "| pearson_corr | 0.611379861831665 | 0.611379861831665 | 0.611379861831665 |\n",
       "| jensen_shannon_divergence | 1.3229633569717407 | 1.3229633569717407 | 1.3229633569717407 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-glossary",
   "metadata": {},
   "source": [
    "### 0.01, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "waiting-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5754036903381348 | 0.5754036903381348 | 0.5754036903381348 |\n",
       "| pearson_corr | 0.5168760418891907 | 0.5168760418891907 | 0.5168760418891907 |\n",
       "| jensen_shannon_divergence | 1.4924464225769043 | 1.4924464225769043 | 1.4924464225769043 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4638281464576721 | 0.4638281464576721 | 0.4638281464576721 |\n",
       "| pearson_corr | 0.6322964429855347 | 0.6322964429855347 | 0.6322964429855347 |\n",
       "| jensen_shannon_divergence | 1.3287949562072754 | 1.3287949562072754 | 1.3287949562072754 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6021310687065125 | 0.6021310687065125 | 0.6021310687065125 |\n",
       "| pearson_corr | 0.5154244899749756 | 0.5154244899749756 | 0.5154244899749756 |\n",
       "| jensen_shannon_divergence | 1.6080148220062256 | 1.6080148220062256 | 1.6080148220062256 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6472703218460083 | 0.6472703218460083 | 0.6472703218460083 |\n",
       "| pearson_corr | 0.3543688952922821 | 0.3543688952922821 | 0.3543688952922821 |\n",
       "| jensen_shannon_divergence | 2.0513923168182373 | 2.0513923168182373 | 2.0513923168182373 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5292101502418518 | 0.5292101502418518 | 0.5292101502418518 |\n",
       "| pearson_corr | 0.5924198627471924 | 0.5924198627471924 | 0.5924198627471924 |\n",
       "| jensen_shannon_divergence | 1.5824764966964722 | 1.5824764966964722 | 1.5824764966964722 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
