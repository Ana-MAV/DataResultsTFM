{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fleet-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos paquetes\n",
    "import sys\n",
    "sys.path.append('Src/')\n",
    "from data_modificado import * #hay funciones que estan cambiadas en este script para adaptralas a nuestro dataset\n",
    "from train_2 import * #este hubo que modificar una linea tambien\n",
    "from transfer_learning import * #hubo que modificart lo mismo que en train_2\n",
    "from test_functions import *\n",
    "from layers import *\n",
    "from utils import *\n",
    "from loss import *\n",
    "from metric import *\n",
    "from results import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import itertools as it\n",
    "\n",
    "#funciones\n",
    "def read_df(\n",
    "              metadata_names=['age','Temperature','Precipitation3Days'],\n",
    "              random_state=42,\n",
    "              otu_filename='../Datasets/otu_table_all_80.csv',\n",
    "              metadata_filename='../Datasets/metadata_table_all_80.csv'):\n",
    "    otu = pd.read_csv(otu_filename, index_col=0, header=None).T\n",
    "    #print(otu.head())\n",
    "    otu = otu.set_index('otuids')\n",
    "    otu = otu.astype('int32')\n",
    "    metadata = pd.read_csv(metadata_filename)\n",
    "    #print(metadata.head())\n",
    "    metadata = metadata.set_index('X.SampleID')\n",
    "    metadata.head()\n",
    "    domain = metadata[metadata_names]\n",
    "    #if 'INBREDS' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['INBREDS'], prefix='INBREDS')], axis=1)\n",
    "    #    domain = domain.drop(['INBREDS'], axis=1)\n",
    "    #elif 'Maize_Line' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['Maize_Line'], prefix='Maize_Line')], axis=1)\n",
    "    #    domain = domain.drop(['Maize_Line'], axis=1) \n",
    "    df = pd.concat([otu, domain], axis=1, sort=True, join='outer')\n",
    "    #print(df.head())\n",
    "    #data_microbioma = df[otu.columns].to_numpy(dtype=np.float32)\n",
    "    #data_domain = df[domain.columns].to_numpy(dtype=np.float32)\n",
    "    df_microbioma = df[otu.columns]\n",
    "    df_domain = df[domain.columns]\n",
    "    df_domain.head()\n",
    "    df_microbioma_train, df_microbioma_no_train, df_domain_train, df_domain_no_train = \\\n",
    "        train_test_split(df_microbioma, df_domain, test_size=0.1, random_state=random_state)\n",
    "    # Transfer learning subset\n",
    "    df_microbioma_test, df_microbioma_transfer_learning, df_domain_test, df_domain_transfer_learning = \\\n",
    "        train_test_split(df_microbioma_no_train, df_domain_no_train, test_size=0.1, random_state=random_state)\n",
    "    df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test = \\\n",
    "        train_test_split(df_microbioma_transfer_learning, df_domain_transfer_learning, test_size=0.3, random_state=random_state)\n",
    "    \n",
    "    return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "    #return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "\n",
    "\n",
    "def train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                batch_size, epochs, train_callbacks):\n",
    "    all_models = model_fn()\n",
    "    model, encoder_bioma, encoder_domain, decoder_bioma = all_models\n",
    "    metrics_prefix = None\n",
    "    if encoder_bioma is not None and encoder_domain is not None:\n",
    "        x_train = (m_train, d_train)\n",
    "        y_train = (m_train, m_train, z_train)\n",
    "        x_test = (m_test, d_test)\n",
    "        y_test = (m_test, m_test, z_test)\n",
    "    elif encoder_bioma is not None:\n",
    "        x_train = m_train\n",
    "        y_train = m_train\n",
    "        x_test = m_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'bioma'\n",
    "    elif encoder_domain is not None:\n",
    "        x_train = d_train\n",
    "        y_train = m_train\n",
    "        x_test = d_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'domain'\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(\n",
    "        batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    r = model.fit(train_dataset,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=val_dataset,\n",
    "                  callbacks=train_callbacks,\n",
    "                  verbose=0)\n",
    "    if metrics_prefix is not None:\n",
    "        old_keys = r.history\n",
    "        r.history = {}\n",
    "        for k, v in old_keys.items():\n",
    "            if k == 'loss' or k == 'val_loss':\n",
    "                new_key = k\n",
    "            elif k.startswith('val_'):\n",
    "                new_key = 'val_{}_{}'.format(metrics_prefix, k[4:])\n",
    "            else:\n",
    "                new_key = '{}_{}'.format(metrics_prefix, k)\n",
    "            r.history[new_key] = v\n",
    "    del val_dataset\n",
    "    del train_dataset\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del x_test\n",
    "    del y_test\n",
    "    return r, all_models\n",
    "\n",
    "def train_2(model_fn,\n",
    "          data_microbioma,\n",
    "          data_domain,\n",
    "          latent_space=10,\n",
    "          folds=5,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          learning_rate_scheduler=ExpDecayScheluder(),\n",
    "          random_seed=347,\n",
    "          verbose=0):\n",
    "    data_zeros_latent = np.zeros((data_microbioma.shape[0], latent_space), dtype=data_microbioma.dtype)\n",
    "    results = []\n",
    "    models = []\n",
    "    train_callbacks = [\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=epochs + 1, restore_best_weights=True)]\n",
    "    if verbose >= 0:\n",
    "        train_callbacks += [TqdmCallback(verbose=verbose)]\n",
    "    if learning_rate_scheduler is not None:\n",
    "        train_callbacks += [learning_rate_scheduler.make()]\n",
    "\n",
    "    if folds <= 1:\n",
    "        m_train, m_test = data_microbioma, data_microbioma\n",
    "        d_train, d_test = data_domain, data_domain\n",
    "        z_train, z_test = data_zeros_latent, data_zeros_latent\n",
    "        tf.random.set_seed(random_seed)\n",
    "        r, m = train_kfold(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                           batch_size, epochs, train_callbacks)\n",
    "        results.append(r)\n",
    "        models.append(m)\n",
    "\n",
    "    else: #EL PROBLEMA ESTA AQUI, QUE HACE FALTA UN \n",
    "        kf = KFold(n_splits=folds, random_state=random_seed, shuffle=True)\n",
    "        tf.random.set_seed(random_seed)\n",
    "\n",
    "        for train_index, test_index in kf.split(data_microbioma):\n",
    "            m_train, m_test = data_microbioma[train_index], data_microbioma[test_index]\n",
    "            #print(m_train)\n",
    "            #d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            if data_domain is None:\n",
    "                d_train, d_test = None, None\n",
    "            else:\n",
    "                d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            #print(d_train)\n",
    "            #Esto de hacer el if else ha funcionado, pero no se si hace lo que debe bien\n",
    "            z_train, z_test = data_zeros_latent[train_index], data_zeros_latent[test_index]\n",
    "            r, m = train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                               batch_size, epochs, train_callbacks)\n",
    "            results.append(r)\n",
    "            models.append(m)\n",
    "    return results, models\n",
    "\n",
    "def perform_experiment_2_mod(cv_folds, epochs, batch_size, learning_rate, optimizer,\n",
    "                       learning_rate_scheduler, input_transform, output_transform,\n",
    "                       reconstruction_loss, latent_space, layers,\n",
    "                       activation, activation_latent,\n",
    "                       data_microbioma_train, data_domain_train,\n",
    "                       show_results=True, device='/CPU:0'): #Show results cambiado de False  aTrue\n",
    "    if input_transform is not None:\n",
    "        input_transform = input_transform()\n",
    "    #----------    \n",
    "    if output_transform is not None:\n",
    "        output_transform = output_transform()\n",
    "    #----------      \n",
    "    if reconstruction_loss.__class__.__name__ == 'MakeLoss':\n",
    "        reconstruction_loss = reconstruction_loss.make()\n",
    "    else:\n",
    "        reconstruction_loss = reconstruction_loss()\n",
    "    domain_layers = [l // 16 for l in layers] ####que es esto???? Esto es para las capas del domain\n",
    "    #print(domain_layers)\n",
    "    bioma_autoencoder = \" -> \".join([\"b\"] +\n",
    "                                    [str(l) for l in layers] +\n",
    "                                    [str(latent_space)] +\n",
    "                                    [str(l) for l in reversed(layers)] +\n",
    "                                    [\"b\"])\n",
    "    #---------- \n",
    "    #esto solo se utiliza para el texto, es irrelevante para nuestro error\n",
    "    if data_domain_train is not None:\n",
    "        domain_autoencoder = \" -> \".join([\"d\"] +\n",
    "                                     [str(l) for l in domain_layers] +\n",
    "                                     [str(latent_space)] +\n",
    "                                     [str(l) for l in reversed(layers)] +\n",
    "                                     [\"b\"])\n",
    "        \n",
    "    else: \n",
    "        domain_autoencoder = \" \"\n",
    "    #---------- \n",
    "    #donde se usa domain autoencoder?\n",
    "    in_transform_name = input_transform.__class__.__name__ if input_transform else \"none\"\n",
    "    out_transform_name = output_transform.__class__.__name__ if output_transform else \"none\"\n",
    "    lr_scheduler_text = learning_rate_scheduler[\n",
    "        1] if learning_rate_scheduler is not None else \"none\"\n",
    "    lr_text = learning_rate if learning_rate_scheduler is not None else \"constant = {}\".format(\n",
    "        learning_rate)\n",
    "    learning_rate_scheduler = learning_rate_scheduler[\n",
    "        0] if learning_rate_scheduler is not None else None\n",
    "    optimizer = optimizer(learning_rate=learning_rate)\n",
    "    #---------- \n",
    "    experiment_parameters = [\n",
    "        (\"Input transform\", in_transform_name),\n",
    "        (\"Output transform\", out_transform_name),\n",
    "        (\"Reconstruction Loss\", reconstruction_loss.__class__.__name__),\n",
    "        (\"Latent Space\", latent_space),\n",
    "        (\"Bioma Autoencoder\", bioma_autoencoder),\n",
    "        (\"Domain Autoencoder\", domain_autoencoder),\n",
    "        (\"Activation Encoder\", activation),\n",
    "        (\"Activation Decoder\", activation),\n",
    "        (\"Activation Latent\", activation_latent),\n",
    "        (\"CV folds\", cv_folds),\n",
    "        (\"Epochs\", epochs),\n",
    "        (\"Batch Size\", batch_size),\n",
    "        (\"Learning Rate Scheduler\", lr_scheduler_text),\n",
    "        (\"Learning Rate\", lr_text),\n",
    "        (\"Optimizer\", optimizer.__class__.__name__),\n",
    "    ]\n",
    "    #----------  \n",
    "    if show_results:\n",
    "        md_text = \"\"\n",
    "        md_text += \"| Parameter             | Value         |\\n\"\n",
    "        md_text += \"|:----------------------|:--------------|\\n\"\n",
    "        for n, v in experiment_parameters:\n",
    "            md_text += \"| {} | {} |\\n\".format(n, v)\n",
    "\n",
    "        display(Markdown(md_text))\n",
    "    #------------\n",
    "    def create_model(print_data=False):\n",
    "        bioma_shape=data_microbioma_train.shape[1]\n",
    "        \n",
    "        if data_domain_train is not None:\n",
    "            domain_shape=data_domain_train.shape[1]\n",
    "            #print(\"data_domain_train!=None\")\n",
    "        else:\n",
    "            domain_shape=None\n",
    "            #print(\"data_domain_train==None\")\n",
    "        models = autoencoder(bioma_shape=bioma_shape,\n",
    "                             #bioma_shape=717,\n",
    "                             domain_shape=domain_shape,\n",
    "                             output_shape=bioma_shape,\n",
    "                             #output_shape=717,\n",
    "                             latent_space=latent_space,\n",
    "                             bioma_layers=layers, #Esto es lo de [512,316]\n",
    "                             domain_layers=domain_layers, #Esto son cada una de las layers divididas por 16\n",
    "                             input_transform=input_transform,\n",
    "                             output_transform=output_transform,\n",
    "                             activation_function_encoder=activation,\n",
    "                             activation_function_decoder=activation,\n",
    "                             activation_function_latent=activation_latent)\n",
    "        #Entiendo analizando lo demas que aqui NO esta el error\n",
    "        #la funcion autoencoder esta en model.py (es la unica funcion en ese script)\n",
    "        \n",
    "        model, encoder_bioma, encoder_domain, decoder_bioma = models\n",
    "\n",
    "        if print_data:\n",
    "            plot_models(model, encoder_bioma, encoder_domain, decoder_bioma)\n",
    "        compile_train(model,\n",
    "                      encoder_bioma=encoder_bioma,\n",
    "                      encoder_domain=encoder_domain,\n",
    "                      reconstruction_error=reconstruction_loss,\n",
    "                      encoded_comparison_error=losses.MeanAbsoluteError(),\n",
    "                      metrics=get_experiment_metrics(input_transform, output_transform),\n",
    "                      optimizer=optimizer)\n",
    "        \n",
    "        #print(\"He acabado create_model :)\")\n",
    "        return model, encoder_bioma, encoder_domain, decoder_bioma\n",
    "    #-----------\n",
    "    create_model(print_data=False)\n",
    "    #-----------\n",
    "    #Esta en esta seccion el problema, en train_2\n",
    "    #print(data_domain_train)\n",
    "    #print(latent_space)\n",
    "    with tf.device(device):\n",
    "        results, models = train_2(create_model,\n",
    "                                data_microbioma_train,\n",
    "                                data_domain_train,\n",
    "                                latent_space=latent_space,\n",
    "                                folds=cv_folds,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                learning_rate_scheduler=learning_rate_scheduler,\n",
    "                                verbose=-1)\n",
    "    #----------\n",
    "    validation_results = print_results(results, show_results=show_results)\n",
    "    if show_results:\n",
    "        display(Markdown(\"*************\"))\n",
    "\n",
    "    return experiment_parameters + validation_results, models, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-baking",
   "metadata": {},
   "source": [
    "__params:__\n",
    " - \"activat_func\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"activ_ouput\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"learning_rate\":[0.01,0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "august-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "#nombres_metadatos = [\"KCAL\",\"PROT\",\"TFAT\",\"CARB\",\"MOIS\",\"ALC\",\"CAFF\",\"THEO\",\"SUGR\",\"FIBE\",\"CALC\",\"IRON\",\"MAGN\",\"PHOS\",\"POTA\",\"SODI\",\"ZINC\",\"COPP\",\"SELE\",\"VC\",\"VB1\",\"VB2\",\"NIAC\",\"VB6\",\"FOLA\",\"VB12\",\"VARA\",\"RET\",\"BCAR\",\"ACAR\",\"CRYP\",\"LYCO\",\"LZ\",\"ATOC\",\"VK\",\"CHOLE\",\"SFAT\",\"MFAT\",\"PFAT\",\"VITD\",\"CHOLN\"]\n",
    "nombres_metadatos = ['F_TOTAL', 'V_TOTAL', 'V_REDOR_TOTAL', 'V_STARCHY_TOTAL', 'V_LEGUMES', 'G_TOTAL', 'PF_TOTAL', 'PF_MEAT', 'PF_CUREDMEAT', 'PF_ORGAN', 'PF_POULT',\\\n",
    "\t\t\t\t\t'PF_SEAFD_HI', 'PF_SEAFD_LOW', 'PF_EGGS', 'PF_SOY', 'PF_NUTSDS', 'D_TOTAL', 'D_YOGURT', 'OILS', 'SOLID_FATS', 'ADD_SUGARS', 'A_DRINKS']\n",
    "df_microbioma_train, df_microbioma_test, _, _, \\\n",
    "df_domain_train, df_domain_test, _, _, otu_columns, domain_columns = read_df(metadata_names=nombres_metadatos,otu_filename='resultados_ana/datos_otus_metadatos_especies/otu_table_especies_80.csv',metadata_filename='datos-remoto/comidas/metadatos_comidas.csv')\n",
    "\n",
    "data_microbioma_train = df_microbioma_train.to_numpy(dtype=np.float32)\n",
    "data_microbioma_test = df_microbioma_test.to_numpy(dtype=np.float32)\n",
    "data_domain_train = df_domain_train.to_numpy(dtype=np.float32)\n",
    "data_domain_test = df_domain_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "#Preparamos las combinaciones pertinentes (5 mejores)\n",
    "combinations = [[100,64,0.01,optimizers.Adam,15,[512,256],\"tanh\",\"softmax\"],\\\n",
    "                [100,64,0.001,optimizers.Adam,15,[512,256,128],\"tanh\",\"tanh\"],\\\n",
    "                [100,96,0.01,optimizers.Adam,15,[512,256],\"tanh\",\"tanh\"],\\\n",
    "                [100,64,0.001,optimizers.Adam,15,[512,256],\"tanh\",\"tanh\"],\\\n",
    "                [100,64,0.01,optimizers.Adam,15,[512,256,128],\"tanh\",\"tanh\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-attraction",
   "metadata": {},
   "source": [
    "### 0.001, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "owned-integrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.454685240983963 | 0.454685240983963 | 0.454685240983963 |\n",
       "| pearson_corr | 0.6571277976036072 | 0.6571277976036072 | 0.6571277976036072 |\n",
       "| jensen_shannon_divergence | 1.1736319065093994 | 1.1736319065093994 | 1.1736319065093994 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46914058923721313 | 0.46914058923721313 | 0.46914058923721313 |\n",
       "| pearson_corr | 0.6359567642211914 | 0.6359567642211914 | 0.6359567642211914 |\n",
       "| jensen_shannon_divergence | 1.2942585945129395 | 1.2942585945129395 | 1.2942585945129395 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340391516685486 | 0.8340391516685486 | 0.8340391516685486 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 18.018535614013672 | 18.018535614013672 | 18.018535614013672 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4480418860912323 | 0.4480418860912323 | 0.4480418860912323 |\n",
       "| pearson_corr | 0.6422702074050903 | 0.6422702074050903 | 0.6422702074050903 |\n",
       "| jensen_shannon_divergence | 1.234660029411316 | 1.234660029411316 | 1.234660029411316 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4666164815425873 | 0.4666164815425873 | 0.4666164815425873 |\n",
       "| pearson_corr | 0.6362999677658081 | 0.6362999677658081 | 0.6362999677658081 |\n",
       "| jensen_shannon_divergence | 1.3176071643829346 | 1.3176071643829346 | 1.3176071643829346 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation=None)(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-practice",
   "metadata": {},
   "source": [
    "### 0.001, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabulous-biography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4697962701320648 | 0.4697962701320648 | 0.4697962701320648 |\n",
       "| pearson_corr | 0.6278499364852905 | 0.6278499364852905 | 0.6278499364852905 |\n",
       "| jensen_shannon_divergence | 1.2745449542999268 | 1.2745449542999268 | 1.2745449542999268 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.556059718132019 | 0.556059718132019 | 0.556059718132019 |\n",
       "| pearson_corr | 0.4581071436405182 | 0.4581071436405182 | 0.4581071436405182 |\n",
       "| jensen_shannon_divergence | 1.7942365407943726 | 1.7942365407943726 | 1.7942365407943726 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340297341346741 | 0.8340297341346741 | 0.8340297341346741 |\n",
       "| pearson_corr | 0.4550006091594696 | 0.4550006091594696 | 0.4550006091594696 |\n",
       "| jensen_shannon_divergence | 14.509845733642578 | 14.509845733642578 | 14.509845733642578 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7368212938308716 | 0.7368212938308716 | 0.7368212938308716 |\n",
       "| pearson_corr | 0.19605056941509247 | 0.19605056941509247 | 0.19605056941509247 |\n",
       "| jensen_shannon_divergence | 2.685950994491577 | 2.685950994491577 | 2.685950994491577 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5507296919822693 | 0.5507296919822693 | 0.5507296919822693 |\n",
       "| pearson_corr | 0.4892386496067047 | 0.4892386496067047 | 0.4892386496067047 |\n",
       "| jensen_shannon_divergence | 1.7643920183181763 | 1.7643920183181763 | 1.7643920183181763 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-calculation",
   "metadata": {},
   "source": [
    "### 0.001, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "worst-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4557536244392395 | 0.4557536244392395 | 0.4557536244392395 |\n",
       "| pearson_corr | 0.6583699584007263 | 0.6583699584007263 | 0.6583699584007263 |\n",
       "| jensen_shannon_divergence | 1.0955818891525269 | 1.0955818891525269 | 1.0955818891525269 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5663546919822693 | 0.5663546919822693 | 0.5663546919822693 |\n",
       "| pearson_corr | 0.506542980670929 | 0.506542980670929 | 0.506542980670929 |\n",
       "| jensen_shannon_divergence | 1.701059103012085 | 1.701059103012085 | 1.701059103012085 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.855878829956055 | 17.855878829956055 | 17.855878829956055 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9328562021255493 | 0.9328562021255493 | 0.9328562021255493 |\n",
       "| pearson_corr | 0.04094476252794266 | 0.04094476252794266 | 0.04094476252794266 |\n",
       "| jensen_shannon_divergence | 6.396814346313477 | 6.396814346313477 | 6.396814346313477 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47546452283859253 | 0.47546452283859253 | 0.47546452283859253 |\n",
       "| pearson_corr | 0.6414799094200134 | 0.6414799094200134 | 0.6414799094200134 |\n",
       "| jensen_shannon_divergence | 1.2747626304626465 | 1.2747626304626465 | 1.2747626304626465 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-midwest",
   "metadata": {},
   "source": [
    "### 0.001, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hazardous-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5541794300079346 | 0.5541794300079346 | 0.5541794300079346 |\n",
       "| pearson_corr | 0.5340099930763245 | 0.5340099930763245 | 0.5340099930763245 |\n",
       "| jensen_shannon_divergence | 1.474056601524353 | 1.474056601524353 | 1.474056601524353 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6012593507766724 | 0.6012593507766724 | 0.6012593507766724 |\n",
       "| pearson_corr | 0.4188355505466461 | 0.4188355505466461 | 0.4188355505466461 |\n",
       "| jensen_shannon_divergence | 2.1614794731140137 | 2.1614794731140137 | 2.1614794731140137 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.493024826049805 | 17.493024826049805 | 17.493024826049805 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.90387362241745 | 0.90387362241745 | 0.90387362241745 |\n",
       "| pearson_corr | 0.013990704901516438 | 0.013990704901516438 | 0.013990704901516438 |\n",
       "| jensen_shannon_divergence | 5.880156993865967 | 5.880156993865967 | 5.880156993865967 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4614819586277008 | 0.4614819586277008 | 0.4614819586277008 |\n",
       "| pearson_corr | 0.6427099108695984 | 0.6427099108695984 | 0.6427099108695984 |\n",
       "| jensen_shannon_divergence | 1.2181636095046997 | 1.2181636095046997 | 1.2181636095046997 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-reservation",
   "metadata": {},
   "source": [
    "### 0.001, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worse-arabic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46563082933425903 | 0.46563082933425903 | 0.46563082933425903 |\n",
       "| pearson_corr | 0.6393532156944275 | 0.6393532156944275 | 0.6393532156944275 |\n",
       "| jensen_shannon_divergence | 1.1635428667068481 | 1.1635428667068481 | 1.1635428667068481 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4692496359348297 | 0.4692496359348297 | 0.4692496359348297 |\n",
       "| pearson_corr | 0.6532752513885498 | 0.6532752513885498 | 0.6532752513885498 |\n",
       "| jensen_shannon_divergence | 1.1966464519500732 | 1.1966464519500732 | 1.1966464519500732 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340391516685486 | 0.8340391516685486 | 0.8340391516685486 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.984817504882812 | 17.984817504882812 | 17.984817504882812 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47084692120552063 | 0.47084692120552063 | 0.47084692120552063 |\n",
       "| pearson_corr | 0.6478751301765442 | 0.6478751301765442 | 0.6478751301765442 |\n",
       "| jensen_shannon_divergence | 1.1896946430206299 | 1.1896946430206299 | 1.1896946430206299 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47612541913986206 | 0.47612541913986206 | 0.47612541913986206 |\n",
       "| pearson_corr | 0.6437716484069824 | 0.6437716484069824 | 0.6437716484069824 |\n",
       "| jensen_shannon_divergence | 1.2784225940704346 | 1.2784225940704346 | 1.2784225940704346 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "  \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-cleaners",
   "metadata": {},
   "source": [
    "### 0.001, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "numerical-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4665317237377167 | 0.4665317237377167 | 0.4665317237377167 |\n",
       "| pearson_corr | 0.6362316608428955 | 0.6362316608428955 | 0.6362316608428955 |\n",
       "| jensen_shannon_divergence | 1.161092758178711 | 1.161092758178711 | 1.161092758178711 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5593356490135193 | 0.5593356490135193 | 0.5593356490135193 |\n",
       "| pearson_corr | 0.45343345403671265 | 0.45343345403671265 | 0.45343345403671265 |\n",
       "| jensen_shannon_divergence | 1.810833215713501 | 1.810833215713501 | 1.810833215713501 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340297341346741 | 0.8340297341346741 | 0.8340297341346741 |\n",
       "| pearson_corr | 0.4550006091594696 | 0.4550006091594696 | 0.4550006091594696 |\n",
       "| jensen_shannon_divergence | 14.486788749694824 | 14.486788749694824 | 14.486788749694824 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7378708720207214 | 0.7378708720207214 | 0.7378708720207214 |\n",
       "| pearson_corr | 0.19353416562080383 | 0.19353416562080383 | 0.19353416562080383 |\n",
       "| jensen_shannon_divergence | 2.696521282196045 | 2.696521282196045 | 2.696521282196045 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5842398405075073 | 0.5842398405075073 | 0.5842398405075073 |\n",
       "| pearson_corr | 0.3718990087509155 | 0.3718990087509155 | 0.3718990087509155 |\n",
       "| jensen_shannon_divergence | 2.004403591156006 | 2.004403591156006 | 2.004403591156006 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-lloyd",
   "metadata": {},
   "source": [
    "### 0.001, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hundred-titanium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47102269530296326 | 0.47102269530296326 | 0.47102269530296326 |\n",
       "| pearson_corr | 0.6319741606712341 | 0.6319741606712341 | 0.6319741606712341 |\n",
       "| jensen_shannon_divergence | 1.170753836631775 | 1.170753836631775 | 1.170753836631775 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5725802779197693 | 0.5725802779197693 | 0.5725802779197693 |\n",
       "| pearson_corr | 0.5160364508628845 | 0.5160364508628845 | 0.5160364508628845 |\n",
       "| jensen_shannon_divergence | 1.803995966911316 | 1.803995966911316 | 1.803995966911316 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.816612243652344 | 17.816612243652344 | 17.816612243652344 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9326993227005005 | 0.9326993227005005 | 0.9326993227005005 |\n",
       "| pearson_corr | 0.04080675169825554 | 0.04080675169825554 | 0.04080675169825554 |\n",
       "| jensen_shannon_divergence | 6.45945405960083 | 6.45945405960083 | 6.45945405960083 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5075467824935913 | 0.5075467824935913 | 0.5075467824935913 |\n",
       "| pearson_corr | 0.575332760810852 | 0.575332760810852 | 0.575332760810852 |\n",
       "| jensen_shannon_divergence | 1.466222882270813 | 1.466222882270813 | 1.466222882270813 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-engineering",
   "metadata": {},
   "source": [
    "### 0.001, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "palestinian-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5546322464942932 | 0.5546322464942932 | 0.5546322464942932 |\n",
       "| pearson_corr | 0.5266302227973938 | 0.5266302227973938 | 0.5266302227973938 |\n",
       "| jensen_shannon_divergence | 1.5469964742660522 | 1.5469964742660522 | 1.5469964742660522 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6083794236183167 | 0.6083794236183167 | 0.6083794236183167 |\n",
       "| pearson_corr | 0.35432904958724976 | 0.35432904958724976 | 0.35432904958724976 |\n",
       "| jensen_shannon_divergence | 2.382486581802368 | 2.382486581802368 | 2.382486581802368 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.49144172668457 | 17.49144172668457 | 17.49144172668457 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8970406651496887 | 0.8970406651496887 | 0.8970406651496887 |\n",
       "| pearson_corr | 0.014060079120099545 | 0.014060079120099545 | 0.014060079120099545 |\n",
       "| jensen_shannon_divergence | 5.609100818634033 | 5.609100818634033 | 5.609100818634033 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4738336503505707 | 0.4738336503505707 | 0.4738336503505707 |\n",
       "| pearson_corr | 0.6432756185531616 | 0.6432756185531616 | 0.6432756185531616 |\n",
       "| jensen_shannon_divergence | 1.2693103551864624 | 1.2693103551864624 | 1.2693103551864624 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-techno",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alternate-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45747217535972595 | 0.45747217535972595 | 0.45747217535972595 |\n",
       "| pearson_corr | 0.6556234359741211 | 0.6556234359741211 | 0.6556234359741211 |\n",
       "| jensen_shannon_divergence | 1.1557211875915527 | 1.1557211875915527 | 1.1557211875915527 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4422742426395416 | 0.4422742426395416 | 0.4422742426395416 |\n",
       "| pearson_corr | 0.6739092469215393 | 0.6739092469215393 | 0.6739092469215393 |\n",
       "| jensen_shannon_divergence | 1.1276943683624268 | 1.1276943683624268 | 1.1276943683624268 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340391516685486 | 0.8340391516685486 | 0.8340391516685486 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 18.0137939453125 | 18.0137939453125 | 18.0137939453125 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4553718566894531 | 0.4553718566894531 | 0.4553718566894531 |\n",
       "| pearson_corr | 0.6790469884872437 | 0.6790469884872437 | 0.6790469884872437 |\n",
       "| jensen_shannon_divergence | 1.1338437795639038 | 1.1338437795639038 | 1.1338437795639038 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.44137483835220337 | 0.44137483835220337 | 0.44137483835220337 |\n",
       "| pearson_corr | 0.6618361473083496 | 0.6618361473083496 | 0.6618361473083496 |\n",
       "| jensen_shannon_divergence | 1.1526650190353394 | 1.1526650190353394 | 1.1526650190353394 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-fruit",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "starting-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45397478342056274 | 0.45397478342056274 | 0.45397478342056274 |\n",
       "| pearson_corr | 0.6457430124282837 | 0.6457430124282837 | 0.6457430124282837 |\n",
       "| jensen_shannon_divergence | 1.1297980546951294 | 1.1297980546951294 | 1.1297980546951294 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5576670169830322 | 0.5576670169830322 | 0.5576670169830322 |\n",
       "| pearson_corr | 0.45517078042030334 | 0.45517078042030334 | 0.45517078042030334 |\n",
       "| jensen_shannon_divergence | 1.8031333684921265 | 1.8031333684921265 | 1.8031333684921265 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340297341346741 | 0.8340297341346741 | 0.8340297341346741 |\n",
       "| pearson_corr | 0.4550006091594696 | 0.4550006091594696 | 0.4550006091594696 |\n",
       "| jensen_shannon_divergence | 14.509708404541016 | 14.509708404541016 | 14.509708404541016 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7366084456443787 | 0.7366084456443787 | 0.7366084456443787 |\n",
       "| pearson_corr | 0.19626650214195251 | 0.19626650214195251 | 0.19626650214195251 |\n",
       "| jensen_shannon_divergence | 2.6834235191345215 | 2.6834235191345215 | 2.6834235191345215 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5642772316932678 | 0.5642772316932678 | 0.5642772316932678 |\n",
       "| pearson_corr | 0.4488489329814911 | 0.4488489329814911 | 0.4488489329814911 |\n",
       "| jensen_shannon_divergence | 1.8086730241775513 | 1.8086730241775513 | 1.8086730241775513 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-makeup",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "inner-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4664815068244934 | 0.4664815068244934 | 0.4664815068244934 |\n",
       "| pearson_corr | 0.6362637281417847 | 0.6362637281417847 | 0.6362637281417847 |\n",
       "| jensen_shannon_divergence | 1.1609796285629272 | 1.1609796285629272 | 1.1609796285629272 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5666792988777161 | 0.5666792988777161 | 0.5666792988777161 |\n",
       "| pearson_corr | 0.505897581577301 | 0.505897581577301 | 0.505897581577301 |\n",
       "| jensen_shannon_divergence | 1.7043668031692505 | 1.7043668031692505 | 1.7043668031692505 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.856632232666016 | 17.856632232666016 | 17.856632232666016 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9318560361862183 | 0.9318560361862183 | 0.9318560361862183 |\n",
       "| pearson_corr | 0.0410352423787117 | 0.0410352423787117 | 0.0410352423787117 |\n",
       "| jensen_shannon_divergence | 6.380636692047119 | 6.380636692047119 | 6.380636692047119 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4761494994163513 | 0.4761494994163513 | 0.4761494994163513 |\n",
       "| pearson_corr | 0.6415034532546997 | 0.6415034532546997 | 0.6415034532546997 |\n",
       "| jensen_shannon_divergence | 1.282346487045288 | 1.282346487045288 | 1.282346487045288 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-inspiration",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vietnamese-treasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5278299450874329 | 0.5278299450874329 | 0.5278299450874329 |\n",
       "| pearson_corr | 0.5684719681739807 | 0.5684719681739807 | 0.5684719681739807 |\n",
       "| jensen_shannon_divergence | 1.4807896614074707 | 1.4807896614074707 | 1.4807896614074707 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6064213514328003 | 0.6064213514328003 | 0.6064213514328003 |\n",
       "| pearson_corr | 0.3562266528606415 | 0.3562266528606415 | 0.3562266528606415 |\n",
       "| jensen_shannon_divergence | 2.3637566566467285 | 2.3637566566467285 | 2.3637566566467285 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.492002487182617 | 17.492002487182617 | 17.492002487182617 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8966017961502075 | 0.8966017961502075 | 0.8966017961502075 |\n",
       "| pearson_corr | 0.012280460447072983 | 0.012280460447072983 | 0.012280460447072983 |\n",
       "| jensen_shannon_divergence | 5.622940540313721 | 5.622940540313721 | 5.622940540313721 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45009055733680725 | 0.45009055733680725 | 0.45009055733680725 |\n",
       "| pearson_corr | 0.6579956412315369 | 0.6579956412315369 | 0.6579956412315369 |\n",
       "| jensen_shannon_divergence | 1.1670845746994019 | 1.1670845746994019 | 1.1670845746994019 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-criterion",
   "metadata": {},
   "source": [
    "### 0.001, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "smaller-vessel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46638593077659607 | 0.46638593077659607 | 0.46638593077659607 |\n",
       "| pearson_corr | 0.6370236277580261 | 0.6370236277580261 | 0.6370236277580261 |\n",
       "| jensen_shannon_divergence | 1.1609570980072021 | 1.1609570980072021 | 1.1609570980072021 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.461480975151062 | 0.461480975151062 | 0.461480975151062 |\n",
       "| pearson_corr | 0.6363386511802673 | 0.6363386511802673 | 0.6363386511802673 |\n",
       "| jensen_shannon_divergence | 1.1980254650115967 | 1.1980254650115967 | 1.1980254650115967 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340391516685486 | 0.8340391516685486 | 0.8340391516685486 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 18.011022567749023 | 18.011022567749023 | 18.011022567749023 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45312073826789856 | 0.45312073826789856 | 0.45312073826789856 |\n",
       "| pearson_corr | 0.6614310145378113 | 0.6614310145378113 | 0.6614310145378113 |\n",
       "| jensen_shannon_divergence | 1.1950252056121826 | 1.1950252056121826 | 1.1950252056121826 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46054816246032715 | 0.46054816246032715 | 0.46054816246032715 |\n",
       "| pearson_corr | 0.6686617136001587 | 0.6686617136001587 | 0.6686617136001587 |\n",
       "| jensen_shannon_divergence | 1.2272322177886963 | 1.2272322177886963 | 1.2272322177886963 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-constitution",
   "metadata": {},
   "source": [
    "### 0.001, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sexual-acting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4613914489746094 | 0.4613914489746094 | 0.4613914489746094 |\n",
       "| pearson_corr | 0.6383143663406372 | 0.6383143663406372 | 0.6383143663406372 |\n",
       "| jensen_shannon_divergence | 1.1752426624298096 | 1.1752426624298096 | 1.1752426624298096 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5642277598381042 | 0.5642277598381042 | 0.5642277598381042 |\n",
       "| pearson_corr | 0.4400530457496643 | 0.4400530457496643 | 0.4400530457496643 |\n",
       "| jensen_shannon_divergence | 1.8292793035507202 | 1.8292793035507202 | 1.8292793035507202 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340296745300293 | 0.8340296745300293 | 0.8340296745300293 |\n",
       "| pearson_corr | 0.4550006091594696 | 0.4550006091594696 | 0.4550006091594696 |\n",
       "| jensen_shannon_divergence | 14.50147533416748 | 14.50147533416748 | 14.50147533416748 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7379387617111206 | 0.7379387617111206 | 0.7379387617111206 |\n",
       "| pearson_corr | 0.19344083964824677 | 0.19344083964824677 | 0.19344083964824677 |\n",
       "| jensen_shannon_divergence | 2.6994400024414062 | 2.6994400024414062 | 2.6994400024414062 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5687854290008545 | 0.5687854290008545 | 0.5687854290008545 |\n",
       "| pearson_corr | 0.4418941140174866 | 0.4418941140174866 | 0.4418941140174866 |\n",
       "| jensen_shannon_divergence | 1.7503644227981567 | 1.7503644227981567 | 1.7503644227981567 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-pasta",
   "metadata": {},
   "source": [
    "### 0.001, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "excellent-pencil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4746127128601074 | 0.4746127128601074 | 0.4746127128601074 |\n",
       "| pearson_corr | 0.614115834236145 | 0.614115834236145 | 0.614115834236145 |\n",
       "| jensen_shannon_divergence | 1.2168242931365967 | 1.2168242931365967 | 1.2168242931365967 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7652083039283752 | 0.7652083039283752 | 0.7652083039283752 |\n",
       "| pearson_corr | 0.19453591108322144 | 0.19453591108322144 | 0.19453591108322144 |\n",
       "| jensen_shannon_divergence | 3.058349609375 | 3.058349609375 | 3.058349609375 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.86199188232422 | 17.86199188232422 | 17.86199188232422 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9411401152610779 | 0.9411401152610779 | 0.9411401152610779 |\n",
       "| pearson_corr | 0.040984466671943665 | 0.040984466671943665 | 0.040984466671943665 |\n",
       "| jensen_shannon_divergence | 6.587149620056152 | 6.587149620056152 | 6.587149620056152 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4579387903213501 | 0.4579387903213501 | 0.4579387903213501 |\n",
       "| pearson_corr | 0.6551989316940308 | 0.6551989316940308 | 0.6551989316940308 |\n",
       "| jensen_shannon_divergence | 1.2411296367645264 | 1.2411296367645264 | 1.2411296367645264 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-agenda",
   "metadata": {},
   "source": [
    "### 0.001, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greatest-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4670194387435913 | 0.4670194387435913 | 0.4670194387435913 |\n",
       "| pearson_corr | 0.6340781450271606 | 0.6340781450271606 | 0.6340781450271606 |\n",
       "| jensen_shannon_divergence | 1.1637259721755981 | 1.1637259721755981 | 1.1637259721755981 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5697749257087708 | 0.5697749257087708 | 0.5697749257087708 |\n",
       "| pearson_corr | 0.48187023401260376 | 0.48187023401260376 | 0.48187023401260376 |\n",
       "| jensen_shannon_divergence | 1.7360210418701172 | 1.7360210418701172 | 1.7360210418701172 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.902545928955078 | 17.902545928955078 | 17.902545928955078 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9397351145744324 | 0.9397351145744324 | 0.9397351145744324 |\n",
       "| pearson_corr | 0.031139478087425232 | 0.031139478087425232 | 0.031139478087425232 |\n",
       "| jensen_shannon_divergence | 7.123992919921875 | 7.123992919921875 | 7.123992919921875 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4862757623195648 | 0.4862757623195648 | 0.4862757623195648 |\n",
       "| pearson_corr | 0.5879261493682861 | 0.5879261493682861 | 0.5879261493682861 |\n",
       "| jensen_shannon_divergence | 1.3685411214828491 | 1.3685411214828491 | 1.3685411214828491 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-airfare",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "### 0.01, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "universal-command",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5617742538452148 | 0.5617742538452148 | 0.5617742538452148 |\n",
       "| pearson_corr | 0.5154846906661987 | 0.5154846906661987 | 0.5154846906661987 |\n",
       "| jensen_shannon_divergence | 1.7900068759918213 | 1.7900068759918213 | 1.7900068759918213 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5053883790969849 | 0.5053883790969849 | 0.5053883790969849 |\n",
       "| pearson_corr | 0.5888961553573608 | 0.5888961553573608 | 0.5888961553573608 |\n",
       "| jensen_shannon_divergence | 1.5625436305999756 | 1.5625436305999756 | 1.5625436305999756 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340391516685486 | 0.8340391516685486 | 0.8340391516685486 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 18.01313591003418 | 18.01313591003418 | 18.01313591003418 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47268038988113403 | 0.47268038988113403 | 0.47268038988113403 |\n",
       "| pearson_corr | 0.6234410405158997 | 0.6234410405158997 | 0.6234410405158997 |\n",
       "| jensen_shannon_divergence | 1.3180394172668457 | 1.3180394172668457 | 1.3180394172668457 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48353663086891174 | 0.48353663086891174 | 0.48353663086891174 |\n",
       "| pearson_corr | 0.6115251183509827 | 0.6115251183509827 | 0.6115251183509827 |\n",
       "| jensen_shannon_divergence | 1.3780310153961182 | 1.3780310153961182 | 1.3780310153961182 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-laptop",
   "metadata": {},
   "source": [
    "### 0.01, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "grateful-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4632151126861572 | 0.4632151126861572 | 0.4632151126861572 |\n",
       "| pearson_corr | 0.6462752819061279 | 0.6462752819061279 | 0.6462752819061279 |\n",
       "| jensen_shannon_divergence | 1.195075273513794 | 1.195075273513794 | 1.195075273513794 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.561447024345398 | 0.561447024345398 | 0.561447024345398 |\n",
       "| pearson_corr | 0.4474167227745056 | 0.4474167227745056 | 0.4474167227745056 |\n",
       "| jensen_shannon_divergence | 1.8302079439163208 | 1.8302079439163208 | 1.8302079439163208 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340299129486084 | 0.8340299129486084 | 0.8340299129486084 |\n",
       "| pearson_corr | 0.4550006091594696 | 0.4550006091594696 | 0.4550006091594696 |\n",
       "| jensen_shannon_divergence | 14.513056755065918 | 14.513056755065918 | 14.513056755065918 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7372086048126221 | 0.7372086048126221 | 0.7372086048126221 |\n",
       "| pearson_corr | 0.19552472233772278 | 0.19552472233772278 | 0.19552472233772278 |\n",
       "| jensen_shannon_divergence | 2.6937975883483887 | 2.6937975883483887 | 2.6937975883483887 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5858666300773621 | 0.5858666300773621 | 0.5858666300773621 |\n",
       "| pearson_corr | 0.3661733567714691 | 0.3661733567714691 | 0.3661733567714691 |\n",
       "| jensen_shannon_divergence | 2.01344895362854 | 2.01344895362854 | 2.01344895362854 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-adapter",
   "metadata": {},
   "source": [
    "### 0.01, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hidden-latino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4595654606819153 | 0.4595654606819153 | 0.4595654606819153 |\n",
       "| pearson_corr | 0.6455560326576233 | 0.6455560326576233 | 0.6455560326576233 |\n",
       "| jensen_shannon_divergence | 1.1773000955581665 | 1.1773000955581665 | 1.1773000955581665 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5629740953445435 | 0.5629740953445435 | 0.5629740953445435 |\n",
       "| pearson_corr | 0.5126970410346985 | 0.5126970410346985 | 0.5126970410346985 |\n",
       "| jensen_shannon_divergence | 1.692236065864563 | 1.692236065864563 | 1.692236065864563 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.85792350769043 | 17.85792350769043 | 17.85792350769043 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9418675899505615 | 0.9418675899505615 | 0.9418675899505615 |\n",
       "| pearson_corr | 0.040319982916116714 | 0.040319982916116714 | 0.040319982916116714 |\n",
       "| jensen_shannon_divergence | 6.654928207397461 | 6.654928207397461 | 6.654928207397461 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47274428606033325 | 0.47274428606033325 | 0.47274428606033325 |\n",
       "| pearson_corr | 0.6449876427650452 | 0.6449876427650452 | 0.6449876427650452 |\n",
       "| jensen_shannon_divergence | 1.2511407136917114 | 1.2511407136917114 | 1.2511407136917114 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-cleaner",
   "metadata": {},
   "source": [
    "### 0.01, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "enormous-vancouver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.517383873462677 | 0.517383873462677 | 0.517383873462677 |\n",
       "| pearson_corr | 0.5927719473838806 | 0.5927719473838806 | 0.5927719473838806 |\n",
       "| jensen_shannon_divergence | 1.2766988277435303 | 1.2766988277435303 | 1.2766988277435303 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5866370797157288 | 0.5866370797157288 | 0.5866370797157288 |\n",
       "| pearson_corr | 0.4139954447746277 | 0.4139954447746277 | 0.4139954447746277 |\n",
       "| jensen_shannon_divergence | 1.9916139841079712 | 1.9916139841079712 | 1.9916139841079712 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.484933853149414 | 17.484933853149414 | 17.484933853149414 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9141695499420166 | 0.9141695499420166 | 0.9141695499420166 |\n",
       "| pearson_corr | 0.018536848947405815 | 0.018536848947405815 | 0.018536848947405815 |\n",
       "| jensen_shannon_divergence | 6.099287986755371 | 6.099287986755371 | 6.099287986755371 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46212702989578247 | 0.46212702989578247 | 0.46212702989578247 |\n",
       "| pearson_corr | 0.6424729228019714 | 0.6424729228019714 | 0.6424729228019714 |\n",
       "| jensen_shannon_divergence | 1.2203289270401 | 1.2203289270401 | 1.2203289270401 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-youth",
   "metadata": {},
   "source": [
    "### 0.01, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "systematic-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4647634029388428 | 0.4647634029388428 | 0.4647634029388428 |\n",
       "| pearson_corr | 0.6454776525497437 | 0.6454776525497437 | 0.6454776525497437 |\n",
       "| jensen_shannon_divergence | 1.2330526113510132 | 1.2330526113510132 | 1.2330526113510132 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4680565893650055 | 0.4680565893650055 | 0.4680565893650055 |\n",
       "| pearson_corr | 0.6517784595489502 | 0.6517784595489502 | 0.6517784595489502 |\n",
       "| jensen_shannon_divergence | 1.2223402261734009 | 1.2223402261734009 | 1.2223402261734009 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340391516685486 | 0.8340391516685486 | 0.8340391516685486 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 18.01503562927246 | 18.01503562927246 | 18.01503562927246 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4759161174297333 | 0.4759161174297333 | 0.4759161174297333 |\n",
       "| pearson_corr | 0.6487429738044739 | 0.6487429738044739 | 0.6487429738044739 |\n",
       "| jensen_shannon_divergence | 1.2074581384658813 | 1.2074581384658813 | 1.2074581384658813 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47397366166114807 | 0.47397366166114807 | 0.47397366166114807 |\n",
       "| pearson_corr | 0.6307224631309509 | 0.6307224631309509 | 0.6307224631309509 |\n",
       "| jensen_shannon_divergence | 1.3619205951690674 | 1.3619205951690674 | 1.3619205951690674 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-puzzle",
   "metadata": {},
   "source": [
    "### 0.01, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sorted-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4660753309726715 | 0.4660753309726715 | 0.4660753309726715 |\n",
       "| pearson_corr | 0.6379039287567139 | 0.6379039287567139 | 0.6379039287567139 |\n",
       "| jensen_shannon_divergence | 1.1618186235427856 | 1.1618186235427856 | 1.1618186235427856 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5576560497283936 | 0.5576560497283936 | 0.5576560497283936 |\n",
       "| pearson_corr | 0.4549061357975006 | 0.4549061357975006 | 0.4549061357975006 |\n",
       "| jensen_shannon_divergence | 1.804214358329773 | 1.804214358329773 | 1.804214358329773 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340298533439636 | 0.8340298533439636 | 0.8340298533439636 |\n",
       "| pearson_corr | 0.4550006091594696 | 0.4550006091594696 | 0.4550006091594696 |\n",
       "| jensen_shannon_divergence | 14.511395454406738 | 14.511395454406738 | 14.511395454406738 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7369203567504883 | 0.7369203567504883 | 0.7369203567504883 |\n",
       "| pearson_corr | 0.19597004354000092 | 0.19597004354000092 | 0.19597004354000092 |\n",
       "| jensen_shannon_divergence | 2.687187433242798 | 2.687187433242798 | 2.687187433242798 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5820274949073792 | 0.5820274949073792 | 0.5820274949073792 |\n",
       "| pearson_corr | 0.37781408429145813 | 0.37781408429145813 | 0.37781408429145813 |\n",
       "| jensen_shannon_divergence | 1.9769774675369263 | 1.9769774675369263 | 1.9769774675369263 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-train",
   "metadata": {},
   "source": [
    "### 0.01, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "creative-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46679913997650146 | 0.46679913997650146 | 0.46679913997650146 |\n",
       "| pearson_corr | 0.6362395882606506 | 0.6362395882606506 | 0.6362395882606506 |\n",
       "| jensen_shannon_divergence | 1.1605443954467773 | 1.1605443954467773 | 1.1605443954467773 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5650662183761597 | 0.5650662183761597 | 0.5650662183761597 |\n",
       "| pearson_corr | 0.5089272260665894 | 0.5089272260665894 | 0.5089272260665894 |\n",
       "| jensen_shannon_divergence | 1.6966928243637085 | 1.6966928243637085 | 1.6966928243637085 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.857995986938477 | 17.857995986938477 | 17.857995986938477 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9327383637428284 | 0.9327383637428284 | 0.9327383637428284 |\n",
       "| pearson_corr | 0.040948815643787384 | 0.040948815643787384 | 0.040948815643787384 |\n",
       "| jensen_shannon_divergence | 6.393174648284912 | 6.393174648284912 | 6.393174648284912 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47535544633865356 | 0.47535544633865356 | 0.47535544633865356 |\n",
       "| pearson_corr | 0.6416867971420288 | 0.6416867971420288 | 0.6416867971420288 |\n",
       "| jensen_shannon_divergence | 1.2739310264587402 | 1.2739310264587402 | 1.2739310264587402 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-diagram",
   "metadata": {},
   "source": [
    "### 0.01, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "coordinate-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5295590162277222 | 0.5295590162277222 | 0.5295590162277222 |\n",
       "| pearson_corr | 0.5695142149925232 | 0.5695142149925232 | 0.5695142149925232 |\n",
       "| jensen_shannon_divergence | 1.469673752784729 | 1.469673752784729 | 1.469673752784729 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6068662405014038 | 0.6068662405014038 | 0.6068662405014038 |\n",
       "| pearson_corr | 0.3576679229736328 | 0.3576679229736328 | 0.3576679229736328 |\n",
       "| jensen_shannon_divergence | 2.3620259761810303 | 2.3620259761810303 | 2.3620259761810303 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.490802764892578 | 17.490802764892578 | 17.490802764892578 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8979804515838623 | 0.8979804515838623 | 0.8979804515838623 |\n",
       "| pearson_corr | 0.013940428383648396 | 0.013940428383648396 | 0.013940428383648396 |\n",
       "| jensen_shannon_divergence | 5.633476257324219 | 5.633476257324219 | 5.633476257324219 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46198296546936035 | 0.46198296546936035 | 0.46198296546936035 |\n",
       "| pearson_corr | 0.6428178548812866 | 0.6428178548812866 | 0.6428178548812866 |\n",
       "| jensen_shannon_divergence | 1.2191208600997925 | 1.2191208600997925 | 1.2191208600997925 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-measure",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "precise-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4733496308326721 | 0.4733496308326721 | 0.4733496308326721 |\n",
       "| pearson_corr | 0.637529194355011 | 0.637529194355011 | 0.637529194355011 |\n",
       "| jensen_shannon_divergence | 1.218363642692566 | 1.218363642692566 | 1.218363642692566 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.458151638507843 | 0.458151638507843 | 0.458151638507843 |\n",
       "| pearson_corr | 0.6447821259498596 | 0.6447821259498596 | 0.6447821259498596 |\n",
       "| jensen_shannon_divergence | 1.2322393655776978 | 1.2322393655776978 | 1.2322393655776978 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340391516685486 | 0.8340391516685486 | 0.8340391516685486 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 18.01536750793457 | 18.01536750793457 | 18.01536750793457 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.44985049962997437 | 0.44985049962997437 | 0.44985049962997437 |\n",
       "| pearson_corr | 0.6702932119369507 | 0.6702932119369507 | 0.6702932119369507 |\n",
       "| jensen_shannon_divergence | 1.1815143823623657 | 1.1815143823623657 | 1.1815143823623657 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.44749554991722107 | 0.44749554991722107 | 0.44749554991722107 |\n",
       "| pearson_corr | 0.6613298654556274 | 0.6613298654556274 | 0.6613298654556274 |\n",
       "| jensen_shannon_divergence | 1.2083559036254883 | 1.2083559036254883 | 1.2083559036254883 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-committee",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "textile-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46780824661254883 | 0.46780824661254883 | 0.46780824661254883 |\n",
       "| pearson_corr | 0.6285131573677063 | 0.6285131573677063 | 0.6285131573677063 |\n",
       "| jensen_shannon_divergence | 1.232012391090393 | 1.232012391090393 | 1.232012391090393 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5575839281082153 | 0.5575839281082153 | 0.5575839281082153 |\n",
       "| pearson_corr | 0.457095205783844 | 0.457095205783844 | 0.457095205783844 |\n",
       "| jensen_shannon_divergence | 1.7937710285186768 | 1.7937710285186768 | 1.7937710285186768 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340299129486084 | 0.8340299129486084 | 0.8340299129486084 |\n",
       "| pearson_corr | 0.4550006091594696 | 0.4550006091594696 | 0.4550006091594696 |\n",
       "| jensen_shannon_divergence | 14.513117790222168 | 14.513117790222168 | 14.513117790222168 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7397936582565308 | 0.7397936582565308 | 0.7397936582565308 |\n",
       "| pearson_corr | 0.19165857136249542 | 0.19165857136249542 | 0.19165857136249542 |\n",
       "| jensen_shannon_divergence | 2.7205259799957275 | 2.7205259799957275 | 2.7205259799957275 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.547185480594635 | 0.547185480594635 | 0.547185480594635 |\n",
       "| pearson_corr | 0.4838554263114929 | 0.4838554263114929 | 0.4838554263114929 |\n",
       "| jensen_shannon_divergence | 1.780696153640747 | 1.780696153640747 | 1.780696153640747 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-matthew",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "virtual-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4658677577972412 | 0.4658677577972412 | 0.4658677577972412 |\n",
       "| pearson_corr | 0.6411777138710022 | 0.6411777138710022 | 0.6411777138710022 |\n",
       "| jensen_shannon_divergence | 1.159759283065796 | 1.159759283065796 | 1.159759283065796 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5643163323402405 | 0.5643163323402405 | 0.5643163323402405 |\n",
       "| pearson_corr | 0.5100822448730469 | 0.5100822448730469 | 0.5100822448730469 |\n",
       "| jensen_shannon_divergence | 1.6957532167434692 | 1.6957532167434692 | 1.6957532167434692 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.85862922668457 | 17.85862922668457 | 17.85862922668457 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.941926121711731 | 0.941926121711731 | 0.941926121711731 |\n",
       "| pearson_corr | 0.04022732749581337 | 0.04022732749581337 | 0.04022732749581337 |\n",
       "| jensen_shannon_divergence | 6.658896446228027 | 6.658896446228027 | 6.658896446228027 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47357290983200073 | 0.47357290983200073 | 0.47357290983200073 |\n",
       "| pearson_corr | 0.644031286239624 | 0.644031286239624 | 0.644031286239624 |\n",
       "| jensen_shannon_divergence | 1.2585551738739014 | 1.2585551738739014 | 1.2585551738739014 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-cookie",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "thorough-prescription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5410394072532654 | 0.5410394072532654 | 0.5410394072532654 |\n",
       "| pearson_corr | 0.5528478622436523 | 0.5528478622436523 | 0.5528478622436523 |\n",
       "| jensen_shannon_divergence | 1.6689927577972412 | 1.6689927577972412 | 1.6689927577972412 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5996901988983154 | 0.5996901988983154 | 0.5996901988983154 |\n",
       "| pearson_corr | 0.41806650161743164 | 0.41806650161743164 | 0.41806650161743164 |\n",
       "| jensen_shannon_divergence | 2.151785135269165 | 2.151785135269165 | 2.151785135269165 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340392708778381 | 0.8340392708778381 | 0.8340392708778381 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.49213218688965 | 17.49213218688965 | 17.49213218688965 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9452676177024841 | 0.9452676177024841 | 0.9452676177024841 |\n",
       "| pearson_corr | 0.02232794277369976 | 0.02232794277369976 | 0.02232794277369976 |\n",
       "| jensen_shannon_divergence | 7.387359142303467 | 7.387359142303467 | 7.387359142303467 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46212702989578247 | 0.46212702989578247 | 0.46212702989578247 |\n",
       "| pearson_corr | 0.6424729228019714 | 0.6424729228019714 | 0.6424729228019714 |\n",
       "| jensen_shannon_divergence | 1.2203289270401 | 1.2203289270401 | 1.2203289270401 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-moses",
   "metadata": {},
   "source": [
    "### 0.01, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "helpful-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4704922139644623 | 0.4704922139644623 | 0.4704922139644623 |\n",
       "| pearson_corr | 0.6408110857009888 | 0.6408110857009888 | 0.6408110857009888 |\n",
       "| jensen_shannon_divergence | 1.178073763847351 | 1.178073763847351 | 1.178073763847351 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4674172103404999 | 0.4674172103404999 | 0.4674172103404999 |\n",
       "| pearson_corr | 0.6530334949493408 | 0.6530334949493408 | 0.6530334949493408 |\n",
       "| jensen_shannon_divergence | 1.1862353086471558 | 1.1862353086471558 | 1.1862353086471558 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340390920639038 | 0.8340390920639038 | 0.8340390920639038 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.88174057006836 | 17.88174057006836 | 17.88174057006836 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4307110011577606 | 0.4307110011577606 | 0.4307110011577606 |\n",
       "| pearson_corr | 0.6809374094009399 | 0.6809374094009399 | 0.6809374094009399 |\n",
       "| jensen_shannon_divergence | 1.1341381072998047 | 1.1341381072998047 | 1.1341381072998047 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47644561529159546 | 0.47644561529159546 | 0.47644561529159546 |\n",
       "| pearson_corr | 0.6448391675949097 | 0.6448391675949097 | 0.6448391675949097 |\n",
       "| jensen_shannon_divergence | 1.2816938161849976 | 1.2816938161849976 | 1.2816938161849976 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-distributor",
   "metadata": {},
   "source": [
    "### 0.01, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "yellow-detail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46688681840896606 | 0.46688681840896606 | 0.46688681840896606 |\n",
       "| pearson_corr | 0.6365949511528015 | 0.6365949511528015 | 0.6365949511528015 |\n",
       "| jensen_shannon_divergence | 1.1606014966964722 | 1.1606014966964722 | 1.1606014966964722 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5593084096908569 | 0.5593084096908569 | 0.5593084096908569 |\n",
       "| pearson_corr | 0.4570334553718567 | 0.4570334553718567 | 0.4570334553718567 |\n",
       "| jensen_shannon_divergence | 1.7840198278427124 | 1.7840198278427124 | 1.7840198278427124 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340299129486084 | 0.8340299129486084 | 0.8340299129486084 |\n",
       "| pearson_corr | 0.4550006091594696 | 0.4550006091594696 | 0.4550006091594696 |\n",
       "| jensen_shannon_divergence | 14.514311790466309 | 14.514311790466309 | 14.514311790466309 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7378356456756592 | 0.7378356456756592 | 0.7378356456756592 |\n",
       "| pearson_corr | 0.1950731724500656 | 0.1950731724500656 | 0.1950731724500656 |\n",
       "| jensen_shannon_divergence | 2.69999098777771 | 2.69999098777771 | 2.69999098777771 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5832908153533936 | 0.5832908153533936 | 0.5832908153533936 |\n",
       "| pearson_corr | 0.3743371367454529 | 0.3743371367454529 | 0.3743371367454529 |\n",
       "| jensen_shannon_divergence | 1.9939206838607788 | 1.9939206838607788 | 1.9939206838607788 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-question",
   "metadata": {},
   "source": [
    "### 0.01, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "smart-reception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45293861627578735 | 0.45293861627578735 | 0.45293861627578735 |\n",
       "| pearson_corr | 0.6491031050682068 | 0.6491031050682068 | 0.6491031050682068 |\n",
       "| jensen_shannon_divergence | 1.0918246507644653 | 1.0918246507644653 | 1.0918246507644653 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5668673515319824 | 0.5668673515319824 | 0.5668673515319824 |\n",
       "| pearson_corr | 0.49613624811172485 | 0.49613624811172485 | 0.49613624811172485 |\n",
       "| jensen_shannon_divergence | 1.7410900592803955 | 1.7410900592803955 | 1.7410900592803955 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340393900871277 | 0.8340393900871277 | 0.8340393900871277 |\n",
       "| pearson_corr | 0.4550003409385681 | 0.4550003409385681 | 0.4550003409385681 |\n",
       "| jensen_shannon_divergence | 17.932842254638672 | 17.932842254638672 | 17.932842254638672 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9310094714164734 | 0.9310094714164734 | 0.9310094714164734 |\n",
       "| pearson_corr | 0.04137827828526497 | 0.04137827828526497 | 0.04137827828526497 |\n",
       "| jensen_shannon_divergence | 6.4009528160095215 | 6.4009528160095215 | 6.4009528160095215 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46199896931648254 | 0.46199896931648254 | 0.46199896931648254 |\n",
       "| pearson_corr | 0.6427128314971924 | 0.6427128314971924 | 0.6427128314971924 |\n",
       "| jensen_shannon_divergence | 1.2192420959472656 | 1.2192420959472656 | 1.2192420959472656 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-absence",
   "metadata": {},
   "source": [
    "### 0.01, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "funny-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'softmax']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.517383873462677 | 0.517383873462677 | 0.517383873462677 |\n",
       "| pearson_corr | 0.5927719473838806 | 0.5927719473838806 | 0.5927719473838806 |\n",
       "| jensen_shannon_divergence | 1.2766988277435303 | 1.2766988277435303 | 1.2766988277435303 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5876115560531616 | 0.5876115560531616 | 0.5876115560531616 |\n",
       "| pearson_corr | 0.40740448236465454 | 0.40740448236465454 | 0.40740448236465454 |\n",
       "| jensen_shannon_divergence | 2.02797794342041 | 2.02797794342041 | 2.02797794342041 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8340297341346741 | 0.8340297341346741 | 0.8340297341346741 |\n",
       "| pearson_corr | 0.4550006091594696 | 0.4550006091594696 | 0.4550006091594696 |\n",
       "| jensen_shannon_divergence | 16.1992244720459 | 16.1992244720459 | 16.1992244720459 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8761976361274719 | 0.8761976361274719 | 0.8761976361274719 |\n",
       "| pearson_corr | 0.0486978217959404 | 0.0486978217959404 | 0.0486978217959404 |\n",
       "| jensen_shannon_divergence | 5.578155517578125 | 5.578155517578125 | 5.578155517578125 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46212702989578247 | 0.46212702989578247 | 0.46212702989578247 |\n",
       "| pearson_corr | 0.6424729228019714 | 0.6424729228019714 | 0.6424729228019714 |\n",
       "| jensen_shannon_divergence | 1.2203289270401 | 1.2203289270401 | 1.2203289270401 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=data_domain_train,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
