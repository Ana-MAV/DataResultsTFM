{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "average-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos paquetes\n",
    "import sys\n",
    "sys.path.append('Src/')\n",
    "from data_modificado import * #hay funciones que estan cambiadas en este script para adaptralas a nuestro dataset\n",
    "from train_2 import * #este hubo que modificar una linea tambien\n",
    "from transfer_learning import * #hubo que modificart lo mismo que en train_2\n",
    "from test_functions import *\n",
    "from layers import *\n",
    "from utils import *\n",
    "from loss import *\n",
    "from metric import *\n",
    "from results import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import itertools as it\n",
    "\n",
    "#funciones\n",
    "def read_df(\n",
    "              metadata_names=['age','Temperature','Precipitation3Days'],\n",
    "              random_state=42,\n",
    "              otu_filename='../Datasets/otu_table_all_80.csv',\n",
    "              metadata_filename='../Datasets/metadata_table_all_80.csv'):\n",
    "    otu = pd.read_csv(otu_filename, index_col=0, header=None).T\n",
    "    #print(otu.head())\n",
    "    otu = otu.set_index('otuids')\n",
    "    otu = otu.astype('int32')\n",
    "    metadata = pd.read_csv(metadata_filename)\n",
    "    #print(metadata.head())\n",
    "    metadata = metadata.set_index('X.SampleID')\n",
    "    metadata.head()\n",
    "    domain = metadata[metadata_names]\n",
    "    #if 'INBREDS' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['INBREDS'], prefix='INBREDS')], axis=1)\n",
    "    #    domain = domain.drop(['INBREDS'], axis=1)\n",
    "    #elif 'Maize_Line' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['Maize_Line'], prefix='Maize_Line')], axis=1)\n",
    "    #    domain = domain.drop(['Maize_Line'], axis=1) \n",
    "    df = pd.concat([otu, domain], axis=1, sort=True, join='outer')\n",
    "    #print(df.head())\n",
    "    #data_microbioma = df[otu.columns].to_numpy(dtype=np.float32)\n",
    "    #data_domain = df[domain.columns].to_numpy(dtype=np.float32)\n",
    "    df_microbioma = df[otu.columns]\n",
    "    df_domain = df[domain.columns]\n",
    "    df_domain.head()\n",
    "    df_microbioma_train, df_microbioma_no_train, df_domain_train, df_domain_no_train = \\\n",
    "        train_test_split(df_microbioma, df_domain, test_size=0.1, random_state=random_state)\n",
    "    # Transfer learning subset\n",
    "    df_microbioma_test, df_microbioma_transfer_learning, df_domain_test, df_domain_transfer_learning = \\\n",
    "        train_test_split(df_microbioma_no_train, df_domain_no_train, test_size=0.1, random_state=random_state)\n",
    "    df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test = \\\n",
    "        train_test_split(df_microbioma_transfer_learning, df_domain_transfer_learning, test_size=0.3, random_state=random_state)\n",
    "    \n",
    "    return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "    #return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "\n",
    "\n",
    "def train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                batch_size, epochs, train_callbacks):\n",
    "    all_models = model_fn()\n",
    "    model, encoder_bioma, encoder_domain, decoder_bioma = all_models\n",
    "    metrics_prefix = None\n",
    "    if encoder_bioma is not None and encoder_domain is not None:\n",
    "        x_train = (m_train, d_train)\n",
    "        y_train = (m_train, m_train, z_train)\n",
    "        x_test = (m_test, d_test)\n",
    "        y_test = (m_test, m_test, z_test)\n",
    "    elif encoder_bioma is not None:\n",
    "        x_train = m_train\n",
    "        y_train = m_train\n",
    "        x_test = m_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'bioma'\n",
    "    elif encoder_domain is not None:\n",
    "        x_train = d_train\n",
    "        y_train = m_train\n",
    "        x_test = d_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'domain'\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(\n",
    "        batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    r = model.fit(train_dataset,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=val_dataset,\n",
    "                  callbacks=train_callbacks,\n",
    "                  verbose=0)\n",
    "    if metrics_prefix is not None:\n",
    "        old_keys = r.history\n",
    "        r.history = {}\n",
    "        for k, v in old_keys.items():\n",
    "            if k == 'loss' or k == 'val_loss':\n",
    "                new_key = k\n",
    "            elif k.startswith('val_'):\n",
    "                new_key = 'val_{}_{}'.format(metrics_prefix, k[4:])\n",
    "            else:\n",
    "                new_key = '{}_{}'.format(metrics_prefix, k)\n",
    "            r.history[new_key] = v\n",
    "    del val_dataset\n",
    "    del train_dataset\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del x_test\n",
    "    del y_test\n",
    "    return r, all_models\n",
    "\n",
    "def train_2(model_fn,\n",
    "          data_microbioma,\n",
    "          data_domain,\n",
    "          latent_space=10,\n",
    "          folds=5,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          learning_rate_scheduler=ExpDecayScheluder(),\n",
    "          random_seed=347,\n",
    "          verbose=0):\n",
    "    data_zeros_latent = np.zeros((data_microbioma.shape[0], latent_space), dtype=data_microbioma.dtype)\n",
    "    results = []\n",
    "    models = []\n",
    "    train_callbacks = [\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=epochs + 1, restore_best_weights=True)]\n",
    "    if verbose >= 0:\n",
    "        train_callbacks += [TqdmCallback(verbose=verbose)]\n",
    "    if learning_rate_scheduler is not None:\n",
    "        train_callbacks += [learning_rate_scheduler.make()]\n",
    "\n",
    "    if folds <= 1:\n",
    "        m_train, m_test = data_microbioma, data_microbioma\n",
    "        d_train, d_test = data_domain, data_domain\n",
    "        z_train, z_test = data_zeros_latent, data_zeros_latent\n",
    "        tf.random.set_seed(random_seed)\n",
    "        r, m = train_kfold(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                           batch_size, epochs, train_callbacks)\n",
    "        results.append(r)\n",
    "        models.append(m)\n",
    "\n",
    "    else: #EL PROBLEMA ESTA AQUI, QUE HACE FALTA UN \n",
    "        kf = KFold(n_splits=folds, random_state=random_seed, shuffle=True)\n",
    "        tf.random.set_seed(random_seed)\n",
    "\n",
    "        for train_index, test_index in kf.split(data_microbioma):\n",
    "            m_train, m_test = data_microbioma[train_index], data_microbioma[test_index]\n",
    "            #print(m_train)\n",
    "            #d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            if data_domain is None:\n",
    "                d_train, d_test = None, None\n",
    "            else:\n",
    "                d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            #print(d_train)\n",
    "            #Esto de hacer el if else ha funcionado, pero no se si hace lo que debe bien\n",
    "            z_train, z_test = data_zeros_latent[train_index], data_zeros_latent[test_index]\n",
    "            r, m = train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                               batch_size, epochs, train_callbacks)\n",
    "            results.append(r)\n",
    "            models.append(m)\n",
    "    return results, models\n",
    "\n",
    "def perform_experiment_2_mod(cv_folds, epochs, batch_size, learning_rate, optimizer,\n",
    "                       learning_rate_scheduler, input_transform, output_transform,\n",
    "                       reconstruction_loss, latent_space, layers,\n",
    "                       activation, activation_latent,\n",
    "                       data_microbioma_train, data_domain_train,\n",
    "                       show_results=True, device='/CPU:0'): #Show results cambiado de False  aTrue\n",
    "    if input_transform is not None:\n",
    "        input_transform = input_transform()\n",
    "    #----------    \n",
    "    if output_transform is not None:\n",
    "        output_transform = output_transform()\n",
    "    #----------      \n",
    "    if reconstruction_loss.__class__.__name__ == 'MakeLoss':\n",
    "        reconstruction_loss = reconstruction_loss.make()\n",
    "    else:\n",
    "        reconstruction_loss = reconstruction_loss()\n",
    "    domain_layers = [l // 16 for l in layers] ####que es esto???? Esto es para las capas del domain\n",
    "    #print(domain_layers)\n",
    "    bioma_autoencoder = \" -> \".join([\"b\"] +\n",
    "                                    [str(l) for l in layers] +\n",
    "                                    [str(latent_space)] +\n",
    "                                    [str(l) for l in reversed(layers)] +\n",
    "                                    [\"b\"])\n",
    "    #---------- \n",
    "    #esto solo se utiliza para el texto, es irrelevante para nuestro error\n",
    "    if data_domain_train is not None:\n",
    "        domain_autoencoder = \" -> \".join([\"d\"] +\n",
    "                                     [str(l) for l in domain_layers] +\n",
    "                                     [str(latent_space)] +\n",
    "                                     [str(l) for l in reversed(layers)] +\n",
    "                                     [\"b\"])\n",
    "        \n",
    "    else: \n",
    "        domain_autoencoder = \" \"\n",
    "    #---------- \n",
    "    #donde se usa domain autoencoder?\n",
    "    in_transform_name = input_transform.__class__.__name__ if input_transform else \"none\"\n",
    "    out_transform_name = output_transform.__class__.__name__ if output_transform else \"none\"\n",
    "    lr_scheduler_text = learning_rate_scheduler[\n",
    "        1] if learning_rate_scheduler is not None else \"none\"\n",
    "    lr_text = learning_rate if learning_rate_scheduler is not None else \"constant = {}\".format(\n",
    "        learning_rate)\n",
    "    learning_rate_scheduler = learning_rate_scheduler[\n",
    "        0] if learning_rate_scheduler is not None else None\n",
    "    optimizer = optimizer(learning_rate=learning_rate)\n",
    "    #---------- \n",
    "    experiment_parameters = [\n",
    "        (\"Input transform\", in_transform_name),\n",
    "        (\"Output transform\", out_transform_name),\n",
    "        (\"Reconstruction Loss\", reconstruction_loss.__class__.__name__),\n",
    "        (\"Latent Space\", latent_space),\n",
    "        (\"Bioma Autoencoder\", bioma_autoencoder),\n",
    "        (\"Domain Autoencoder\", domain_autoencoder),\n",
    "        (\"Activation Encoder\", activation),\n",
    "        (\"Activation Decoder\", activation),\n",
    "        (\"Activation Latent\", activation_latent),\n",
    "        (\"CV folds\", cv_folds),\n",
    "        (\"Epochs\", epochs),\n",
    "        (\"Batch Size\", batch_size),\n",
    "        (\"Learning Rate Scheduler\", lr_scheduler_text),\n",
    "        (\"Learning Rate\", lr_text),\n",
    "        (\"Optimizer\", optimizer.__class__.__name__),\n",
    "    ]\n",
    "    #----------  \n",
    "    if show_results:\n",
    "        md_text = \"\"\n",
    "        md_text += \"| Parameter             | Value         |\\n\"\n",
    "        md_text += \"|:----------------------|:--------------|\\n\"\n",
    "        for n, v in experiment_parameters:\n",
    "            md_text += \"| {} | {} |\\n\".format(n, v)\n",
    "\n",
    "        display(Markdown(md_text))\n",
    "    #------------\n",
    "    def create_model(print_data=False):\n",
    "        bioma_shape=data_microbioma_train.shape[1]\n",
    "        \n",
    "        if data_domain_train is not None:\n",
    "            domain_shape=data_domain_train.shape[1]\n",
    "            #print(\"data_domain_train!=None\")\n",
    "        else:\n",
    "            domain_shape=None\n",
    "            #print(\"data_domain_train==None\")\n",
    "        models = autoencoder(bioma_shape=bioma_shape,\n",
    "                             #bioma_shape=717,\n",
    "                             domain_shape=domain_shape,\n",
    "                             output_shape=bioma_shape,\n",
    "                             #output_shape=717,\n",
    "                             latent_space=latent_space,\n",
    "                             bioma_layers=layers, #Esto es lo de [512,316]\n",
    "                             domain_layers=domain_layers, #Esto son cada una de las layers divididas por 16\n",
    "                             input_transform=input_transform,\n",
    "                             output_transform=output_transform,\n",
    "                             activation_function_encoder=activation,\n",
    "                             activation_function_decoder=activation,\n",
    "                             activation_function_latent=activation_latent)\n",
    "        #Entiendo analizando lo demas que aqui NO esta el error\n",
    "        #la funcion autoencoder esta en model.py (es la unica funcion en ese script)\n",
    "        \n",
    "        model, encoder_bioma, encoder_domain, decoder_bioma = models\n",
    "\n",
    "        if print_data:\n",
    "            plot_models(model, encoder_bioma, encoder_domain, decoder_bioma)\n",
    "        compile_train(model,\n",
    "                      encoder_bioma=encoder_bioma,\n",
    "                      encoder_domain=encoder_domain,\n",
    "                      reconstruction_error=reconstruction_loss,\n",
    "                      encoded_comparison_error=losses.MeanAbsoluteError(),\n",
    "                      metrics=get_experiment_metrics(input_transform, output_transform),\n",
    "                      optimizer=optimizer)\n",
    "        \n",
    "        #print(\"He acabado create_model :)\")\n",
    "        return model, encoder_bioma, encoder_domain, decoder_bioma\n",
    "    #-----------\n",
    "    create_model(print_data=False)\n",
    "    #-----------\n",
    "    #Esta en esta seccion el problema, en train_2\n",
    "    #print(data_domain_train)\n",
    "    #print(latent_space)\n",
    "    with tf.device(device):\n",
    "        results, models = train_2(create_model,\n",
    "                                data_microbioma_train,\n",
    "                                data_domain_train,\n",
    "                                latent_space=latent_space,\n",
    "                                folds=cv_folds,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                learning_rate_scheduler=learning_rate_scheduler,\n",
    "                                verbose=-1)\n",
    "    #----------\n",
    "    validation_results = print_results(results, show_results=show_results)\n",
    "    if show_results:\n",
    "        display(Markdown(\"*************\"))\n",
    "\n",
    "    return experiment_parameters + validation_results, models, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-album",
   "metadata": {},
   "source": [
    "__params:__\n",
    " - \"activat_func\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"activ_ouput\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"learning_rate\":[0.01,0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "american-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "#nombres_metadatos = [\"KCAL\",\"PROT\",\"TFAT\",\"CARB\",\"MOIS\",\"ALC\",\"CAFF\",\"THEO\",\"SUGR\",\"FIBE\",\"CALC\",\"IRON\",\"MAGN\",\"PHOS\",\"POTA\",\"SODI\",\"ZINC\",\"COPP\",\"SELE\",\"VC\",\"VB1\",\"VB2\",\"NIAC\",\"VB6\",\"FOLA\",\"VB12\",\"VARA\",\"RET\",\"BCAR\",\"ACAR\",\"CRYP\",\"LYCO\",\"LZ\",\"ATOC\",\"VK\",\"CHOLE\",\"SFAT\",\"MFAT\",\"PFAT\",\"VITD\",\"CHOLN\"]\n",
    "nombres_metadatos = [\"F_TOTAL\",\"F_CITMLB\",\"F_OTHER\",\"F_JUICE\",\"V_TOTAL\",\"V_DRKGR\",\"V_REDOR_TOTAL\",\"V_REDOR_TOMATO\",\"V_REDOR_OTHER\",\"V_STARCHY_TOTAL\",\"V_STARCHY_POTATO\",\"V_STARCHY_OTHER\",\"V_OTHER\",\\\n",
    "                     \"V_LEGUMES\",\"G_TOTAL\",\"G_WHOLE\",\"G_REFINED\",\"PF_TOTAL\",\"PF_MPS_TOTAL\",\"PF_MEAT\",\"PF_CUREDMEAT\",\"PF_ORGAN\",\"PF_POULT\",\"PF_SEAFD_HI\",\"PF_SEAFD_LOW\",\"PF_EGGS\",\"PF_SOY\",\"PF_NUTSDS\",\\\n",
    "                     \"PF_LEGUMES\",\"D_TOTAL\",\"D_MILK\",\"D_YOGURT\",\"D_CHEESE\",\"OILS\",\"SOLID_FATS\",\"ADD_SUGARS\",\"A_DRINKS\"]\n",
    "df_microbioma_train, df_microbioma_test, _, _, \\\n",
    "df_domain_train, df_domain_test, _, _, otu_columns, domain_columns = read_df(metadata_names=nombres_metadatos,otu_filename='resultados_ana/datos_otus_metadatos_especies/otu_table_especies_80.csv',metadata_filename='datos-remoto/comidas/metadatos_comidas.csv')\n",
    "\n",
    "data_microbioma_train = df_microbioma_train.to_numpy(dtype=np.float32)\n",
    "data_microbioma_test = df_microbioma_test.to_numpy(dtype=np.float32)\n",
    "data_domain_train = df_domain_train.to_numpy(dtype=np.float32)\n",
    "data_domain_test = df_domain_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "#Preparamos las combinaciones pertinentes (5 mejores)\n",
    "combinations = [[100,64,0.001,optimizers.Adam,15,[512,256],\"tanh\",\"tanh\"],\\\n",
    "                [100,64,0.001,optimizers.Adam,15,[512,256,128],\"tanh\",\"relu\"],\\\n",
    "                [100,96,0.001,optimizers.Adam,15,[512,256],\"tanh\",\"tanh\"],\\\n",
    "                [100,96,0.01,optimizers.Adam,15,[512,256],\"relu\",\"tanh\"],\\\n",
    "                [100,64,0.01,optimizers.Adam,10,[512,256],\"relu\",\"tanh\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-housing",
   "metadata": {},
   "source": [
    "### 0.001, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "capable-metallic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4811488091945648 | 0.4811488091945648 | 0.4811488091945648 |\n",
       "| pearson_corr | 0.5972214937210083 | 0.5972214937210083 | 0.5972214937210083 |\n",
       "| jensen_shannon_divergence | 1.383402705192566 | 1.383402705192566 | 1.383402705192566 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46166253089904785 | 0.46166253089904785 | 0.46166253089904785 |\n",
       "| pearson_corr | 0.6199927926063538 | 0.6199927926063538 | 0.6199927926063538 |\n",
       "| jensen_shannon_divergence | 1.297486424446106 | 1.297486424446106 | 1.297486424446106 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4629550278186798 | 0.4629550278186798 | 0.4629550278186798 |\n",
       "| pearson_corr | 0.6280008554458618 | 0.6280008554458618 | 0.6280008554458618 |\n",
       "| jensen_shannon_divergence | 1.3060563802719116 | 1.3060563802719116 | 1.3060563802719116 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4945608973503113 | 0.4945608973503113 | 0.4945608973503113 |\n",
       "| pearson_corr | 0.5619908571243286 | 0.5619908571243286 | 0.5619908571243286 |\n",
       "| jensen_shannon_divergence | 1.5461465120315552 | 1.5461465120315552 | 1.5461465120315552 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48392412066459656 | 0.48392412066459656 | 0.48392412066459656 |\n",
       "| pearson_corr | 0.5952240228652954 | 0.5952240228652954 | 0.5952240228652954 |\n",
       "| jensen_shannon_divergence | 1.5515658855438232 | 1.5515658855438232 | 1.5515658855438232 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation=None)(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-cradle",
   "metadata": {},
   "source": [
    "### 0.001, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "senior-confidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5985029935836792 | 0.5985029935836792 | 0.5985029935836792 |\n",
       "| pearson_corr | 0.5567106008529663 | 0.5567106008529663 | 0.5567106008529663 |\n",
       "| jensen_shannon_divergence | 1.611543893814087 | 1.611543893814087 | 1.611543893814087 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6299749612808228 | 0.6299749612808228 | 0.6299749612808228 |\n",
       "| pearson_corr | 0.48853686451911926 | 0.48853686451911926 | 0.48853686451911926 |\n",
       "| jensen_shannon_divergence | 1.8700673580169678 | 1.8700673580169678 | 1.8700673580169678 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6247039437294006 | 0.6247039437294006 | 0.6247039437294006 |\n",
       "| pearson_corr | 0.5571277141571045 | 0.5571277141571045 | 0.5571277141571045 |\n",
       "| jensen_shannon_divergence | 1.750973105430603 | 1.750973105430603 | 1.750973105430603 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5830097794532776 | 0.5830097794532776 | 0.5830097794532776 |\n",
       "| pearson_corr | 0.49133726954460144 | 0.49133726954460144 | 0.49133726954460144 |\n",
       "| jensen_shannon_divergence | 1.7394204139709473 | 1.7394204139709473 | 1.7394204139709473 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.50544273853302 | 0.50544273853302 | 0.50544273853302 |\n",
       "| pearson_corr | 0.6039857864379883 | 0.6039857864379883 | 0.6039857864379883 |\n",
       "| jensen_shannon_divergence | 1.4162583351135254 | 1.4162583351135254 | 1.4162583351135254 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-skiing",
   "metadata": {},
   "source": [
    "### 0.001, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cleared-russia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.53293377161026 | 0.53293377161026 | 0.53293377161026 |\n",
       "| pearson_corr | 0.572350263595581 | 0.572350263595581 | 0.572350263595581 |\n",
       "| jensen_shannon_divergence | 1.3118021488189697 | 1.3118021488189697 | 1.3118021488189697 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47123539447784424 | 0.47123539447784424 | 0.47123539447784424 |\n",
       "| pearson_corr | 0.622204601764679 | 0.622204601764679 | 0.622204601764679 |\n",
       "| jensen_shannon_divergence | 1.2678661346435547 | 1.2678661346435547 | 1.2678661346435547 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5713036060333252 | 0.5713036060333252 | 0.5713036060333252 |\n",
       "| pearson_corr | 0.5817434191703796 | 0.5817434191703796 | 0.5817434191703796 |\n",
       "| jensen_shannon_divergence | 1.4483522176742554 | 1.4483522176742554 | 1.4483522176742554 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5557464957237244 | 0.5557464957237244 | 0.5557464957237244 |\n",
       "| pearson_corr | 0.5658727288246155 | 0.5658727288246155 | 0.5658727288246155 |\n",
       "| jensen_shannon_divergence | 1.6183104515075684 | 1.6183104515075684 | 1.6183104515075684 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48849040269851685 | 0.48849040269851685 | 0.48849040269851685 |\n",
       "| pearson_corr | 0.6126341223716736 | 0.6126341223716736 | 0.6126341223716736 |\n",
       "| jensen_shannon_divergence | 1.3767396211624146 | 1.3767396211624146 | 1.3767396211624146 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-concept",
   "metadata": {},
   "source": [
    "### 0.001, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "satisfied-trunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5590741038322449 | 0.5590741038322449 | 0.5590741038322449 |\n",
       "| pearson_corr | 0.5164803862571716 | 0.5164803862571716 | 0.5164803862571716 |\n",
       "| jensen_shannon_divergence | 1.4185982942581177 | 1.4185982942581177 | 1.4185982942581177 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4789031147956848 | 0.4789031147956848 | 0.4789031147956848 |\n",
       "| pearson_corr | 0.605782151222229 | 0.605782151222229 | 0.605782151222229 |\n",
       "| jensen_shannon_divergence | 1.361491322517395 | 1.361491322517395 | 1.361491322517395 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5665954947471619 | 0.5665954947471619 | 0.5665954947471619 |\n",
       "| pearson_corr | 0.5349541306495667 | 0.5349541306495667 | 0.5349541306495667 |\n",
       "| jensen_shannon_divergence | 1.4713943004608154 | 1.4713943004608154 | 1.4713943004608154 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5650014281272888 | 0.5650014281272888 | 0.5650014281272888 |\n",
       "| pearson_corr | 0.5577529072761536 | 0.5577529072761536 | 0.5577529072761536 |\n",
       "| jensen_shannon_divergence | 1.5895720720291138 | 1.5895720720291138 | 1.5895720720291138 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4946209490299225 | 0.4946209490299225 | 0.4946209490299225 |\n",
       "| pearson_corr | 0.5988612771034241 | 0.5988612771034241 | 0.5988612771034241 |\n",
       "| jensen_shannon_divergence | 1.3790714740753174 | 1.3790714740753174 | 1.3790714740753174 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-michigan",
   "metadata": {},
   "source": [
    "### 0.001, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "latin-telling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4739948809146881 | 0.4739948809146881 | 0.4739948809146881 |\n",
       "| pearson_corr | 0.619188666343689 | 0.619188666343689 | 0.619188666343689 |\n",
       "| jensen_shannon_divergence | 1.2129225730895996 | 1.2129225730895996 | 1.2129225730895996 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47678354382514954 | 0.47678354382514954 | 0.47678354382514954 |\n",
       "| pearson_corr | 0.6272305250167847 | 0.6272305250167847 | 0.6272305250167847 |\n",
       "| jensen_shannon_divergence | 1.2687491178512573 | 1.2687491178512573 | 1.2687491178512573 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4843415915966034 | 0.4843415915966034 | 0.4843415915966034 |\n",
       "| pearson_corr | 0.6141325831413269 | 0.6141325831413269 | 0.6141325831413269 |\n",
       "| jensen_shannon_divergence | 1.243565320968628 | 1.243565320968628 | 1.243565320968628 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5380398631095886 | 0.5380398631095886 | 0.5380398631095886 |\n",
       "| pearson_corr | 0.5792554616928101 | 0.5792554616928101 | 0.5792554616928101 |\n",
       "| jensen_shannon_divergence | 1.5231508016586304 | 1.5231508016586304 | 1.5231508016586304 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49160754680633545 | 0.49160754680633545 | 0.49160754680633545 |\n",
       "| pearson_corr | 0.608216404914856 | 0.608216404914856 | 0.608216404914856 |\n",
       "| jensen_shannon_divergence | 1.3598568439483643 | 1.3598568439483643 | 1.3598568439483643 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "  \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-cycle",
   "metadata": {},
   "source": [
    "### 0.001, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "positive-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5873244404792786 | 0.5873244404792786 | 0.5873244404792786 |\n",
       "| pearson_corr | 0.5695104002952576 | 0.5695104002952576 | 0.5695104002952576 |\n",
       "| jensen_shannon_divergence | 1.5207459926605225 | 1.5207459926605225 | 1.5207459926605225 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6448494791984558 | 0.6448494791984558 | 0.6448494791984558 |\n",
       "| pearson_corr | 0.49715346097946167 | 0.49715346097946167 | 0.49715346097946167 |\n",
       "| jensen_shannon_divergence | 1.8698731660842896 | 1.8698731660842896 | 1.8698731660842896 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6173306107521057 | 0.6173306107521057 | 0.6173306107521057 |\n",
       "| pearson_corr | 0.5581023693084717 | 0.5581023693084717 | 0.5581023693084717 |\n",
       "| jensen_shannon_divergence | 1.6748212575912476 | 1.6748212575912476 | 1.6748212575912476 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5904482007026672 | 0.5904482007026672 | 0.5904482007026672 |\n",
       "| pearson_corr | 0.4813525974750519 | 0.4813525974750519 | 0.4813525974750519 |\n",
       "| jensen_shannon_divergence | 1.7280395030975342 | 1.7280395030975342 | 1.7280395030975342 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4960545599460602 | 0.4960545599460602 | 0.4960545599460602 |\n",
       "| pearson_corr | 0.6173783540725708 | 0.6173783540725708 | 0.6173783540725708 |\n",
       "| jensen_shannon_divergence | 1.4189846515655518 | 1.4189846515655518 | 1.4189846515655518 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-perry",
   "metadata": {},
   "source": [
    "### 0.001, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "useful-gamma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5424090623855591 | 0.5424090623855591 | 0.5424090623855591 |\n",
       "| pearson_corr | 0.5868556499481201 | 0.5868556499481201 | 0.5868556499481201 |\n",
       "| jensen_shannon_divergence | 1.3533594608306885 | 1.3533594608306885 | 1.3533594608306885 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4728512465953827 | 0.4728512465953827 | 0.4728512465953827 |\n",
       "| pearson_corr | 0.6275199055671692 | 0.6275199055671692 | 0.6275199055671692 |\n",
       "| jensen_shannon_divergence | 1.2794123888015747 | 1.2794123888015747 | 1.2794123888015747 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5673025250434875 | 0.5673025250434875 | 0.5673025250434875 |\n",
       "| pearson_corr | 0.5836498737335205 | 0.5836498737335205 | 0.5836498737335205 |\n",
       "| jensen_shannon_divergence | 1.4511511325836182 | 1.4511511325836182 | 1.4511511325836182 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5908077955245972 | 0.5908077955245972 | 0.5908077955245972 |\n",
       "| pearson_corr | 0.49918892979621887 | 0.49918892979621887 | 0.49918892979621887 |\n",
       "| jensen_shannon_divergence | 1.6940959692001343 | 1.6940959692001343 | 1.6940959692001343 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4955856502056122 | 0.4955856502056122 | 0.4955856502056122 |\n",
       "| pearson_corr | 0.617967426776886 | 0.617967426776886 | 0.617967426776886 |\n",
       "| jensen_shannon_divergence | 1.4200624227523804 | 1.4200624227523804 | 1.4200624227523804 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-vault",
   "metadata": {},
   "source": [
    "### 0.001, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "exact-fiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5407907366752625 | 0.5407907366752625 | 0.5407907366752625 |\n",
       "| pearson_corr | 0.5861192941665649 | 0.5861192941665649 | 0.5861192941665649 |\n",
       "| jensen_shannon_divergence | 1.3346951007843018 | 1.3346951007843018 | 1.3346951007843018 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4747842848300934 | 0.4747842848300934 | 0.4747842848300934 |\n",
       "| pearson_corr | 0.6299651861190796 | 0.6299651861190796 | 0.6299651861190796 |\n",
       "| jensen_shannon_divergence | 1.283747911453247 | 1.283747911453247 | 1.283747911453247 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5613124370574951 | 0.5613124370574951 | 0.5613124370574951 |\n",
       "| pearson_corr | 0.5853228569030762 | 0.5853228569030762 | 0.5853228569030762 |\n",
       "| jensen_shannon_divergence | 1.409496545791626 | 1.409496545791626 | 1.409496545791626 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5875253677368164 | 0.5875253677368164 | 0.5875253677368164 |\n",
       "| pearson_corr | 0.5046473741531372 | 0.5046473741531372 | 0.5046473741531372 |\n",
       "| jensen_shannon_divergence | 1.683967113494873 | 1.683967113494873 | 1.683967113494873 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5139226913452148 | 0.5139226913452148 | 0.5139226913452148 |\n",
       "| pearson_corr | 0.577754020690918 | 0.577754020690918 | 0.577754020690918 |\n",
       "| jensen_shannon_divergence | 1.4640616178512573 | 1.4640616178512573 | 1.4640616178512573 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-dispatch",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "mounted-lodging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46177345514297485 | 0.46177345514297485 | 0.46177345514297485 |\n",
       "| pearson_corr | 0.6311417818069458 | 0.6311417818069458 | 0.6311417818069458 |\n",
       "| jensen_shannon_divergence | 1.190743327140808 | 1.190743327140808 | 1.190743327140808 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46370911598205566 | 0.46370911598205566 | 0.46370911598205566 |\n",
       "| pearson_corr | 0.6448352932929993 | 0.6448352932929993 | 0.6448352932929993 |\n",
       "| jensen_shannon_divergence | 1.2234668731689453 | 1.2234668731689453 | 1.2234668731689453 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46369698643684387 | 0.46369698643684387 | 0.46369698643684387 |\n",
       "| pearson_corr | 0.6296575665473938 | 0.6296575665473938 | 0.6296575665473938 |\n",
       "| jensen_shannon_divergence | 1.1791839599609375 | 1.1791839599609375 | 1.1791839599609375 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47123515605926514 | 0.47123515605926514 | 0.47123515605926514 |\n",
       "| pearson_corr | 0.6282884478569031 | 0.6282884478569031 | 0.6282884478569031 |\n",
       "| jensen_shannon_divergence | 1.3501132726669312 | 1.3501132726669312 | 1.3501132726669312 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45728108286857605 | 0.45728108286857605 | 0.45728108286857605 |\n",
       "| pearson_corr | 0.6478140354156494 | 0.6478140354156494 | 0.6478140354156494 |\n",
       "| jensen_shannon_divergence | 1.2561354637145996 | 1.2561354637145996 | 1.2561354637145996 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-laptop",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "combined-bradford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5958043336868286 | 0.5958043336868286 | 0.5958043336868286 |\n",
       "| pearson_corr | 0.542673647403717 | 0.542673647403717 | 0.542673647403717 |\n",
       "| jensen_shannon_divergence | 1.5902090072631836 | 1.5902090072631836 | 1.5902090072631836 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6370766162872314 | 0.6370766162872314 | 0.6370766162872314 |\n",
       "| pearson_corr | 0.4928673207759857 | 0.4928673207759857 | 0.4928673207759857 |\n",
       "| jensen_shannon_divergence | 1.8965818881988525 | 1.8965818881988525 | 1.8965818881988525 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6256746053695679 | 0.6256746053695679 | 0.6256746053695679 |\n",
       "| pearson_corr | 0.541942834854126 | 0.541942834854126 | 0.541942834854126 |\n",
       "| jensen_shannon_divergence | 1.7482582330703735 | 1.7482582330703735 | 1.7482582330703735 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5951951742172241 | 0.5951951742172241 | 0.5951951742172241 |\n",
       "| pearson_corr | 0.48521724343299866 | 0.48521724343299866 | 0.48521724343299866 |\n",
       "| jensen_shannon_divergence | 1.736647605895996 | 1.736647605895996 | 1.736647605895996 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5053911805152893 | 0.5053911805152893 | 0.5053911805152893 |\n",
       "| pearson_corr | 0.5947909951210022 | 0.5947909951210022 | 0.5947909951210022 |\n",
       "| jensen_shannon_divergence | 1.429952621459961 | 1.429952621459961 | 1.429952621459961 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-computer",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "announced-planning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5413936972618103 | 0.5413936972618103 | 0.5413936972618103 |\n",
       "| pearson_corr | 0.5917019844055176 | 0.5917019844055176 | 0.5917019844055176 |\n",
       "| jensen_shannon_divergence | 1.3473374843597412 | 1.3473374843597412 | 1.3473374843597412 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4548405110836029 | 0.4548405110836029 | 0.4548405110836029 |\n",
       "| pearson_corr | 0.6536962389945984 | 0.6536962389945984 | 0.6536962389945984 |\n",
       "| jensen_shannon_divergence | 1.200769066810608 | 1.200769066810608 | 1.200769066810608 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5644722580909729 | 0.5644722580909729 | 0.5644722580909729 |\n",
       "| pearson_corr | 0.5871666669845581 | 0.5871666669845581 | 0.5871666669845581 |\n",
       "| jensen_shannon_divergence | 1.433253526687622 | 1.433253526687622 | 1.433253526687622 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5932464599609375 | 0.5932464599609375 | 0.5932464599609375 |\n",
       "| pearson_corr | 0.5024611949920654 | 0.5024611949920654 | 0.5024611949920654 |\n",
       "| jensen_shannon_divergence | 1.693665862083435 | 1.693665862083435 | 1.693665862083435 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49753537774086 | 0.49753537774086 | 0.49753537774086 |\n",
       "| pearson_corr | 0.6138858199119568 | 0.6138858199119568 | 0.6138858199119568 |\n",
       "| jensen_shannon_divergence | 1.426946759223938 | 1.426946759223938 | 1.426946759223938 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-stylus",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "missing-shipping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5419369339942932 | 0.5419369339942932 | 0.5419369339942932 |\n",
       "| pearson_corr | 0.5490684509277344 | 0.5490684509277344 | 0.5490684509277344 |\n",
       "| jensen_shannon_divergence | 1.3445496559143066 | 1.3445496559143066 | 1.3445496559143066 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5003560185432434 | 0.5003560185432434 | 0.5003560185432434 |\n",
       "| pearson_corr | 0.5858258008956909 | 0.5858258008956909 | 0.5858258008956909 |\n",
       "| jensen_shannon_divergence | 1.3576942682266235 | 1.3576942682266235 | 1.3576942682266235 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.562696635723114 | 0.562696635723114 | 0.562696635723114 |\n",
       "| pearson_corr | 0.5381583571434021 | 0.5381583571434021 | 0.5381583571434021 |\n",
       "| jensen_shannon_divergence | 1.4422144889831543 | 1.4422144889831543 | 1.4422144889831543 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5777543783187866 | 0.5777543783187866 | 0.5777543783187866 |\n",
       "| pearson_corr | 0.5346301198005676 | 0.5346301198005676 | 0.5346301198005676 |\n",
       "| jensen_shannon_divergence | 1.6459795236587524 | 1.6459795236587524 | 1.6459795236587524 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4855791926383972 | 0.4855791926383972 | 0.4855791926383972 |\n",
       "| pearson_corr | 0.6193583011627197 | 0.6193583011627197 | 0.6193583011627197 |\n",
       "| jensen_shannon_divergence | 1.368632197380066 | 1.368632197380066 | 1.368632197380066 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-runner",
   "metadata": {},
   "source": [
    "### 0.001, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "transsexual-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45490938425064087 | 0.45490938425064087 | 0.45490938425064087 |\n",
       "| pearson_corr | 0.65250164270401 | 0.65250164270401 | 0.65250164270401 |\n",
       "| jensen_shannon_divergence | 1.1847552061080933 | 1.1847552061080933 | 1.1847552061080933 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47533223032951355 | 0.47533223032951355 | 0.47533223032951355 |\n",
       "| pearson_corr | 0.6154909133911133 | 0.6154909133911133 | 0.6154909133911133 |\n",
       "| jensen_shannon_divergence | 1.3036057949066162 | 1.3036057949066162 | 1.3036057949066162 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4639696478843689 | 0.4639696478843689 | 0.4639696478843689 |\n",
       "| pearson_corr | 0.6161468029022217 | 0.6161468029022217 | 0.6161468029022217 |\n",
       "| jensen_shannon_divergence | 1.2318187952041626 | 1.2318187952041626 | 1.2318187952041626 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4821232259273529 | 0.4821232259273529 | 0.4821232259273529 |\n",
       "| pearson_corr | 0.5963904857635498 | 0.5963904857635498 | 0.5963904857635498 |\n",
       "| jensen_shannon_divergence | 1.5249958038330078 | 1.5249958038330078 | 1.5249958038330078 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4853006601333618 | 0.4853006601333618 | 0.4853006601333618 |\n",
       "| pearson_corr | 0.5867354273796082 | 0.5867354273796082 | 0.5867354273796082 |\n",
       "| jensen_shannon_divergence | 1.4126936197280884 | 1.4126936197280884 | 1.4126936197280884 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-vertex",
   "metadata": {},
   "source": [
    "### 0.001, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "agricultural-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5911540389060974 | 0.5911540389060974 | 0.5911540389060974 |\n",
       "| pearson_corr | 0.5529316663742065 | 0.5529316663742065 | 0.5529316663742065 |\n",
       "| jensen_shannon_divergence | 1.5621495246887207 | 1.5621495246887207 | 1.5621495246887207 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6320650577545166 | 0.6320650577545166 | 0.6320650577545166 |\n",
       "| pearson_corr | 0.4878138303756714 | 0.4878138303756714 | 0.4878138303756714 |\n",
       "| jensen_shannon_divergence | 1.8648624420166016 | 1.8648624420166016 | 1.8648624420166016 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6123591661453247 | 0.6123591661453247 | 0.6123591661453247 |\n",
       "| pearson_corr | 0.554056704044342 | 0.554056704044342 | 0.554056704044342 |\n",
       "| jensen_shannon_divergence | 1.6700538396835327 | 1.6700538396835327 | 1.6700538396835327 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5916973948478699 | 0.5916973948478699 | 0.5916973948478699 |\n",
       "| pearson_corr | 0.47670820355415344 | 0.47670820355415344 | 0.47670820355415344 |\n",
       "| jensen_shannon_divergence | 1.7598716020584106 | 1.7598716020584106 | 1.7598716020584106 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4956240653991699 | 0.4956240653991699 | 0.4956240653991699 |\n",
       "| pearson_corr | 0.616133451461792 | 0.616133451461792 | 0.616133451461792 |\n",
       "| jensen_shannon_divergence | 1.3723628520965576 | 1.3723628520965576 | 1.3723628520965576 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-anthony",
   "metadata": {},
   "source": [
    "### 0.001, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fourth-language",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5343404412269592 | 0.5343404412269592 | 0.5343404412269592 |\n",
       "| pearson_corr | 0.5523898601531982 | 0.5523898601531982 | 0.5523898601531982 |\n",
       "| jensen_shannon_divergence | 1.3671884536743164 | 1.3671884536743164 | 1.3671884536743164 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4655800759792328 | 0.4655800759792328 | 0.4655800759792328 |\n",
       "| pearson_corr | 0.6355234980583191 | 0.6355234980583191 | 0.6355234980583191 |\n",
       "| jensen_shannon_divergence | 1.2343838214874268 | 1.2343838214874268 | 1.2343838214874268 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5518527626991272 | 0.5518527626991272 | 0.5518527626991272 |\n",
       "| pearson_corr | 0.5507520437240601 | 0.5507520437240601 | 0.5507520437240601 |\n",
       "| jensen_shannon_divergence | 1.4133236408233643 | 1.4133236408233643 | 1.4133236408233643 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5688746571540833 | 0.5688746571540833 | 0.5688746571540833 |\n",
       "| pearson_corr | 0.5460275411605835 | 0.5460275411605835 | 0.5460275411605835 |\n",
       "| jensen_shannon_divergence | 1.6438918113708496 | 1.6438918113708496 | 1.6438918113708496 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4810088872909546 | 0.4810088872909546 | 0.4810088872909546 |\n",
       "| pearson_corr | 0.6303095817565918 | 0.6303095817565918 | 0.6303095817565918 |\n",
       "| jensen_shannon_divergence | 1.3459570407867432 | 1.3459570407867432 | 1.3459570407867432 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-northern",
   "metadata": {},
   "source": [
    "### 0.001, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vulnerable-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.538062334060669 | 0.538062334060669 | 0.538062334060669 |\n",
       "| pearson_corr | 0.5440937876701355 | 0.5440937876701355 | 0.5440937876701355 |\n",
       "| jensen_shannon_divergence | 1.3643687963485718 | 1.3643687963485718 | 1.3643687963485718 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4733918309211731 | 0.4733918309211731 | 0.4733918309211731 |\n",
       "| pearson_corr | 0.6060413718223572 | 0.6060413718223572 | 0.6060413718223572 |\n",
       "| jensen_shannon_divergence | 1.3707611560821533 | 1.3707611560821533 | 1.3707611560821533 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5742307901382446 | 0.5742307901382446 | 0.5742307901382446 |\n",
       "| pearson_corr | 0.5424454212188721 | 0.5424454212188721 | 0.5424454212188721 |\n",
       "| jensen_shannon_divergence | 1.4938647747039795 | 1.4938647747039795 | 1.4938647747039795 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5639667510986328 | 0.5639667510986328 | 0.5639667510986328 |\n",
       "| pearson_corr | 0.5511888265609741 | 0.5511888265609741 | 0.5511888265609741 |\n",
       "| jensen_shannon_divergence | 1.6280341148376465 | 1.6280341148376465 | 1.6280341148376465 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48164209723472595 | 0.48164209723472595 | 0.48164209723472595 |\n",
       "| pearson_corr | 0.6272757649421692 | 0.6272757649421692 | 0.6272757649421692 |\n",
       "| jensen_shannon_divergence | 1.351996898651123 | 1.351996898651123 | 1.351996898651123 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-seeking",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "### 0.01, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "entire-knock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47404807806015015 | 0.47404807806015015 | 0.47404807806015015 |\n",
       "| pearson_corr | 0.6398858428001404 | 0.6398858428001404 | 0.6398858428001404 |\n",
       "| jensen_shannon_divergence | 1.1942986249923706 | 1.1942986249923706 | 1.1942986249923706 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4757887125015259 | 0.4757887125015259 | 0.4757887125015259 |\n",
       "| pearson_corr | 0.6303161382675171 | 0.6303161382675171 | 0.6303161382675171 |\n",
       "| jensen_shannon_divergence | 1.2709044218063354 | 1.2709044218063354 | 1.2709044218063354 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46538254618644714 | 0.46538254618644714 | 0.46538254618644714 |\n",
       "| pearson_corr | 0.6563830971717834 | 0.6563830971717834 | 0.6563830971717834 |\n",
       "| jensen_shannon_divergence | 1.1653064489364624 | 1.1653064489364624 | 1.1653064489364624 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5424792766571045 | 0.5424792766571045 | 0.5424792766571045 |\n",
       "| pearson_corr | 0.5685037970542908 | 0.5685037970542908 | 0.5685037970542908 |\n",
       "| jensen_shannon_divergence | 1.5461081266403198 | 1.5461081266403198 | 1.5461081266403198 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47753435373306274 | 0.47753435373306274 | 0.47753435373306274 |\n",
       "| pearson_corr | 0.6324858069419861 | 0.6324858069419861 | 0.6324858069419861 |\n",
       "| jensen_shannon_divergence | 1.3506287336349487 | 1.3506287336349487 | 1.3506287336349487 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-services",
   "metadata": {},
   "source": [
    "### 0.01, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affiliated-september",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5850751996040344 | 0.5850751996040344 | 0.5850751996040344 |\n",
       "| pearson_corr | 0.5691843032836914 | 0.5691843032836914 | 0.5691843032836914 |\n",
       "| jensen_shannon_divergence | 1.5098639726638794 | 1.5098639726638794 | 1.5098639726638794 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6462218165397644 | 0.6462218165397644 | 0.6462218165397644 |\n",
       "| pearson_corr | 0.49212777614593506 | 0.49212777614593506 | 0.49212777614593506 |\n",
       "| jensen_shannon_divergence | 1.8857735395431519 | 1.8857735395431519 | 1.8857735395431519 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6164655685424805 | 0.6164655685424805 | 0.6164655685424805 |\n",
       "| pearson_corr | 0.5592060089111328 | 0.5592060089111328 | 0.5592060089111328 |\n",
       "| jensen_shannon_divergence | 1.6699931621551514 | 1.6699931621551514 | 1.6699931621551514 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5911959409713745 | 0.5911959409713745 | 0.5911959409713745 |\n",
       "| pearson_corr | 0.48108816146850586 | 0.48108816146850586 | 0.48108816146850586 |\n",
       "| jensen_shannon_divergence | 1.7279537916183472 | 1.7279537916183472 | 1.7279537916183472 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49573561549186707 | 0.49573561549186707 | 0.49573561549186707 |\n",
       "| pearson_corr | 0.617778480052948 | 0.617778480052948 | 0.617778480052948 |\n",
       "| jensen_shannon_divergence | 1.4170963764190674 | 1.4170963764190674 | 1.4170963764190674 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-directive",
   "metadata": {},
   "source": [
    "### 0.01, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "intimate-solid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5423393249511719 | 0.5423393249511719 | 0.5423393249511719 |\n",
       "| pearson_corr | 0.5869595408439636 | 0.5869595408439636 | 0.5869595408439636 |\n",
       "| jensen_shannon_divergence | 1.3506145477294922 | 1.3506145477294922 | 1.3506145477294922 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4641821086406708 | 0.4641821086406708 | 0.4641821086406708 |\n",
       "| pearson_corr | 0.639108419418335 | 0.639108419418335 | 0.639108419418335 |\n",
       "| jensen_shannon_divergence | 1.2329154014587402 | 1.2329154014587402 | 1.2329154014587402 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5680671334266663 | 0.5680671334266663 | 0.5680671334266663 |\n",
       "| pearson_corr | 0.5774056911468506 | 0.5774056911468506 | 0.5774056911468506 |\n",
       "| jensen_shannon_divergence | 1.4540966749191284 | 1.4540966749191284 | 1.4540966749191284 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5934001803398132 | 0.5934001803398132 | 0.5934001803398132 |\n",
       "| pearson_corr | 0.48821380734443665 | 0.48821380734443665 | 0.48821380734443665 |\n",
       "| jensen_shannon_divergence | 1.7133556604385376 | 1.7133556604385376 | 1.7133556604385376 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49639520049095154 | 0.49639520049095154 | 0.49639520049095154 |\n",
       "| pearson_corr | 0.6168110370635986 | 0.6168110370635986 | 0.6168110370635986 |\n",
       "| jensen_shannon_divergence | 1.4212965965270996 | 1.4212965965270996 | 1.4212965965270996 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-horror",
   "metadata": {},
   "source": [
    "### 0.01, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "virgin-stock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5467241406440735 | 0.5467241406440735 | 0.5467241406440735 |\n",
       "| pearson_corr | 0.5868820548057556 | 0.5868820548057556 | 0.5868820548057556 |\n",
       "| jensen_shannon_divergence | 1.355027675628662 | 1.355027675628662 | 1.355027675628662 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4823738932609558 | 0.4823738932609558 | 0.4823738932609558 |\n",
       "| pearson_corr | 0.6221296191215515 | 0.6221296191215515 | 0.6221296191215515 |\n",
       "| jensen_shannon_divergence | 1.2773852348327637 | 1.2773852348327637 | 1.2773852348327637 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5856673121452332 | 0.5856673121452332 | 0.5856673121452332 |\n",
       "| pearson_corr | 0.572331964969635 | 0.572331964969635 | 0.572331964969635 |\n",
       "| jensen_shannon_divergence | 1.5096575021743774 | 1.5096575021743774 | 1.5096575021743774 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6472703218460083 | 0.6472703218460083 | 0.6472703218460083 |\n",
       "| pearson_corr | 0.3543688952922821 | 0.3543688952922821 | 0.3543688952922821 |\n",
       "| jensen_shannon_divergence | 2.0513923168182373 | 2.0513923168182373 | 2.0513923168182373 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49307456612586975 | 0.49307456612586975 | 0.49307456612586975 |\n",
       "| pearson_corr | 0.627493143081665 | 0.627493143081665 | 0.627493143081665 |\n",
       "| jensen_shannon_divergence | 1.3783639669418335 | 1.3783639669418335 | 1.3783639669418335 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-outreach",
   "metadata": {},
   "source": [
    "### 0.01, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fundamental-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4786492586135864 | 0.4786492586135864 | 0.4786492586135864 |\n",
       "| pearson_corr | 0.6132789254188538 | 0.6132789254188538 | 0.6132789254188538 |\n",
       "| jensen_shannon_divergence | 1.2083356380462646 | 1.2083356380462646 | 1.2083356380462646 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47249841690063477 | 0.47249841690063477 | 0.47249841690063477 |\n",
       "| pearson_corr | 0.6310031414031982 | 0.6310031414031982 | 0.6310031414031982 |\n",
       "| jensen_shannon_divergence | 1.2593649625778198 | 1.2593649625778198 | 1.2593649625778198 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4718140959739685 | 0.4718140959739685 | 0.4718140959739685 |\n",
       "| pearson_corr | 0.635260820388794 | 0.635260820388794 | 0.635260820388794 |\n",
       "| jensen_shannon_divergence | 1.1785976886749268 | 1.1785976886749268 | 1.1785976886749268 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5338857769966125 | 0.5338857769966125 | 0.5338857769966125 |\n",
       "| pearson_corr | 0.5880791544914246 | 0.5880791544914246 | 0.5880791544914246 |\n",
       "| jensen_shannon_divergence | 1.5115293264389038 | 1.5115293264389038 | 1.5115293264389038 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4926018714904785 | 0.4926018714904785 | 0.4926018714904785 |\n",
       "| pearson_corr | 0.6029317378997803 | 0.6029317378997803 | 0.6029317378997803 |\n",
       "| jensen_shannon_divergence | 1.4062635898590088 | 1.4062635898590088 | 1.4062635898590088 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-explosion",
   "metadata": {},
   "source": [
    "### 0.01, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "animal-drill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5884733200073242 | 0.5884733200073242 | 0.5884733200073242 |\n",
       "| pearson_corr | 0.5692444443702698 | 0.5692444443702698 | 0.5692444443702698 |\n",
       "| jensen_shannon_divergence | 1.527906894683838 | 1.527906894683838 | 1.527906894683838 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6374940872192383 | 0.6374940872192383 | 0.6374940872192383 |\n",
       "| pearson_corr | 0.5107278823852539 | 0.5107278823852539 | 0.5107278823852539 |\n",
       "| jensen_shannon_divergence | 1.8263424634933472 | 1.8263424634933472 | 1.8263424634933472 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6166061758995056 | 0.6166061758995056 | 0.6166061758995056 |\n",
       "| pearson_corr | 0.5591662526130676 | 0.5591662526130676 | 0.5591662526130676 |\n",
       "| jensen_shannon_divergence | 1.6708327531814575 | 1.6708327531814575 | 1.6708327531814575 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5913715362548828 | 0.5913715362548828 | 0.5913715362548828 |\n",
       "| pearson_corr | 0.4817618429660797 | 0.4817618429660797 | 0.4817618429660797 |\n",
       "| jensen_shannon_divergence | 1.725858449935913 | 1.725858449935913 | 1.725858449935913 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49891796708106995 | 0.49891796708106995 | 0.49891796708106995 |\n",
       "| pearson_corr | 0.6055706739425659 | 0.6055706739425659 | 0.6055706739425659 |\n",
       "| jensen_shannon_divergence | 1.429177165031433 | 1.429177165031433 | 1.429177165031433 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-oriental",
   "metadata": {},
   "source": [
    "### 0.01, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "modular-rings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5413151979446411 | 0.5413151979446411 | 0.5413151979446411 |\n",
       "| pearson_corr | 0.5919100046157837 | 0.5919100046157837 | 0.5919100046157837 |\n",
       "| jensen_shannon_divergence | 1.3468934297561646 | 1.3468934297561646 | 1.3468934297561646 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4699172079563141 | 0.4699172079563141 | 0.4699172079563141 |\n",
       "| pearson_corr | 0.6326008439064026 | 0.6326008439064026 | 0.6326008439064026 |\n",
       "| jensen_shannon_divergence | 1.2565138339996338 | 1.2565138339996338 | 1.2565138339996338 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5646067261695862 | 0.5646067261695862 | 0.5646067261695862 |\n",
       "| pearson_corr | 0.5872585773468018 | 0.5872585773468018 | 0.5872585773468018 |\n",
       "| jensen_shannon_divergence | 1.4334784746170044 | 1.4334784746170044 | 1.4334784746170044 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5935360789299011 | 0.5935360789299011 | 0.5935360789299011 |\n",
       "| pearson_corr | 0.5019572377204895 | 0.5019572377204895 | 0.5019572377204895 |\n",
       "| jensen_shannon_divergence | 1.6946351528167725 | 1.6946351528167725 | 1.6946351528167725 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4976891577243805 | 0.4976891577243805 | 0.4976891577243805 |\n",
       "| pearson_corr | 0.6135695576667786 | 0.6135695576667786 | 0.6135695576667786 |\n",
       "| jensen_shannon_divergence | 1.4278128147125244 | 1.4278128147125244 | 1.4278128147125244 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-sleeve",
   "metadata": {},
   "source": [
    "### 0.01, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mounted-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5475392937660217 | 0.5475392937660217 | 0.5475392937660217 |\n",
       "| pearson_corr | 0.5739184617996216 | 0.5739184617996216 | 0.5739184617996216 |\n",
       "| jensen_shannon_divergence | 1.348825216293335 | 1.348825216293335 | 1.348825216293335 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47004926204681396 | 0.47004926204681396 | 0.47004926204681396 |\n",
       "| pearson_corr | 0.6444982290267944 | 0.6444982290267944 | 0.6444982290267944 |\n",
       "| jensen_shannon_divergence | 1.2692550420761108 | 1.2692550420761108 | 1.2692550420761108 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5644980072975159 | 0.5644980072975159 | 0.5644980072975159 |\n",
       "| pearson_corr | 0.5788543224334717 | 0.5788543224334717 | 0.5788543224334717 |\n",
       "| jensen_shannon_divergence | 1.4249824285507202 | 1.4249824285507202 | 1.4249824285507202 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.588610827922821 | 0.588610827922821 | 0.588610827922821 |\n",
       "| pearson_corr | 0.5063248872756958 | 0.5063248872756958 | 0.5063248872756958 |\n",
       "| jensen_shannon_divergence | 1.6878230571746826 | 1.6878230571746826 | 1.6878230571746826 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5047962665557861 | 0.5047962665557861 | 0.5047962665557861 |\n",
       "| pearson_corr | 0.6108266711235046 | 0.6108266711235046 | 0.6108266711235046 |\n",
       "| jensen_shannon_divergence | 1.4369696378707886 | 1.4369696378707886 | 1.4369696378707886 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-portsmouth",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "approximate-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4694780111312866 | 0.4694780111312866 | 0.4694780111312866 |\n",
       "| pearson_corr | 0.6107320785522461 | 0.6107320785522461 | 0.6107320785522461 |\n",
       "| jensen_shannon_divergence | 1.2796586751937866 | 1.2796586751937866 | 1.2796586751937866 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47317367792129517 | 0.47317367792129517 | 0.47317367792129517 |\n",
       "| pearson_corr | 0.6344937086105347 | 0.6344937086105347 | 0.6344937086105347 |\n",
       "| jensen_shannon_divergence | 1.215071439743042 | 1.215071439743042 | 1.215071439743042 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45436546206474304 | 0.45436546206474304 | 0.45436546206474304 |\n",
       "| pearson_corr | 0.6401063799858093 | 0.6401063799858093 | 0.6401063799858093 |\n",
       "| jensen_shannon_divergence | 1.2251265048980713 | 1.2251265048980713 | 1.2251265048980713 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48300933837890625 | 0.48300933837890625 | 0.48300933837890625 |\n",
       "| pearson_corr | 0.6073957681655884 | 0.6073957681655884 | 0.6073957681655884 |\n",
       "| jensen_shannon_divergence | 1.4223181009292603 | 1.4223181009292603 | 1.4223181009292603 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4688187539577484 | 0.4688187539577484 | 0.4688187539577484 |\n",
       "| pearson_corr | 0.6172322034835815 | 0.6172322034835815 | 0.6172322034835815 |\n",
       "| jensen_shannon_divergence | 1.366747260093689 | 1.366747260093689 | 1.366747260093689 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-quebec",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "single-ghost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5890740752220154 | 0.5890740752220154 | 0.5890740752220154 |\n",
       "| pearson_corr | 0.5696137547492981 | 0.5696137547492981 | 0.5696137547492981 |\n",
       "| jensen_shannon_divergence | 1.5313876867294312 | 1.5313876867294312 | 1.5313876867294312 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6424984335899353 | 0.6424984335899353 | 0.6424984335899353 |\n",
       "| pearson_corr | 0.488700270652771 | 0.488700270652771 | 0.488700270652771 |\n",
       "| jensen_shannon_divergence | 1.8871482610702515 | 1.8871482610702515 | 1.8871482610702515 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6171558499336243 | 0.6171558499336243 | 0.6171558499336243 |\n",
       "| pearson_corr | 0.5591491460800171 | 0.5591491460800171 | 0.5591491460800171 |\n",
       "| jensen_shannon_divergence | 1.6741516590118408 | 1.6741516590118408 | 1.6741516590118408 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5903839468955994 | 0.5903839468955994 | 0.5903839468955994 |\n",
       "| pearson_corr | 0.4867982566356659 | 0.4867982566356659 | 0.4867982566356659 |\n",
       "| jensen_shannon_divergence | 1.718055248260498 | 1.718055248260498 | 1.718055248260498 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5017601847648621 | 0.5017601847648621 | 0.5017601847648621 |\n",
       "| pearson_corr | 0.6008924245834351 | 0.6008924245834351 | 0.6008924245834351 |\n",
       "| jensen_shannon_divergence | 1.4431296586990356 | 1.4431296586990356 | 1.4431296586990356 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-cyprus",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "personalized-wedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5428547263145447 | 0.5428547263145447 | 0.5428547263145447 |\n",
       "| pearson_corr | 0.5865005850791931 | 0.5865005850791931 | 0.5865005850791931 |\n",
       "| jensen_shannon_divergence | 1.352586030960083 | 1.352586030960083 | 1.352586030960083 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4765518307685852 | 0.4765518307685852 | 0.4765518307685852 |\n",
       "| pearson_corr | 0.6114801168441772 | 0.6114801168441772 | 0.6114801168441772 |\n",
       "| jensen_shannon_divergence | 1.29989492893219 | 1.29989492893219 | 1.29989492893219 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5672193765640259 | 0.5672193765640259 | 0.5672193765640259 |\n",
       "| pearson_corr | 0.5775068402290344 | 0.5775068402290344 | 0.5775068402290344 |\n",
       "| jensen_shannon_divergence | 1.449827790260315 | 1.449827790260315 | 1.449827790260315 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5933095216751099 | 0.5933095216751099 | 0.5933095216751099 |\n",
       "| pearson_corr | 0.49068745970726013 | 0.49068745970726013 | 0.49068745970726013 |\n",
       "| jensen_shannon_divergence | 1.7087810039520264 | 1.7087810039520264 | 1.7087810039520264 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4918455481529236 | 0.4918455481529236 | 0.4918455481529236 |\n",
       "| pearson_corr | 0.6185665726661682 | 0.6185665726661682 | 0.6185665726661682 |\n",
       "| jensen_shannon_divergence | 1.4012123346328735 | 1.4012123346328735 | 1.4012123346328735 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-plant",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mobile-yahoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5603455901145935 | 0.5603455901145935 | 0.5603455901145935 |\n",
       "| pearson_corr | 0.49182379245758057 | 0.49182379245758057 | 0.49182379245758057 |\n",
       "| jensen_shannon_divergence | 1.4007662534713745 | 1.4007662534713745 | 1.4007662534713745 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4986487925052643 | 0.4986487925052643 | 0.4986487925052643 |\n",
       "| pearson_corr | 0.5774319767951965 | 0.5774319767951965 | 0.5774319767951965 |\n",
       "| jensen_shannon_divergence | 1.4748446941375732 | 1.4748446941375732 | 1.4748446941375732 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5685194134712219 | 0.5685194134712219 | 0.5685194134712219 |\n",
       "| pearson_corr | 0.5206722021102905 | 0.5206722021102905 | 0.5206722021102905 |\n",
       "| jensen_shannon_divergence | 1.4654422998428345 | 1.4654422998428345 | 1.4654422998428345 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5972763299942017 | 0.5972763299942017 | 0.5972763299942017 |\n",
       "| pearson_corr | 0.46558091044425964 | 0.46558091044425964 | 0.46558091044425964 |\n",
       "| jensen_shannon_divergence | 1.8095802068710327 | 1.8095802068710327 | 1.8095802068710327 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48960497975349426 | 0.48960497975349426 | 0.48960497975349426 |\n",
       "| pearson_corr | 0.6106709241867065 | 0.6106709241867065 | 0.6106709241867065 |\n",
       "| jensen_shannon_divergence | 1.3544108867645264 | 1.3544108867645264 | 1.3544108867645264 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-logging",
   "metadata": {},
   "source": [
    "### 0.01, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "moved-rendering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47432711720466614 | 0.47432711720466614 | 0.47432711720466614 |\n",
       "| pearson_corr | 0.6152275800704956 | 0.6152275800704956 | 0.6152275800704956 |\n",
       "| jensen_shannon_divergence | 1.2080706357955933 | 1.2080706357955933 | 1.2080706357955933 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4885191023349762 | 0.4885191023349762 | 0.4885191023349762 |\n",
       "| pearson_corr | 0.600675642490387 | 0.600675642490387 | 0.600675642490387 |\n",
       "| jensen_shannon_divergence | 1.3745545148849487 | 1.3745545148849487 | 1.3745545148849487 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46403783559799194 | 0.46403783559799194 | 0.46403783559799194 |\n",
       "| pearson_corr | 0.6214119791984558 | 0.6214119791984558 | 0.6214119791984558 |\n",
       "| jensen_shannon_divergence | 1.2359544038772583 | 1.2359544038772583 | 1.2359544038772583 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49171438813209534 | 0.49171438813209534 | 0.49171438813209534 |\n",
       "| pearson_corr | 0.5997076630592346 | 0.5997076630592346 | 0.5997076630592346 |\n",
       "| jensen_shannon_divergence | 1.4299862384796143 | 1.4299862384796143 | 1.4299862384796143 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5008096694946289 | 0.5008096694946289 | 0.5008096694946289 |\n",
       "| pearson_corr | 0.5950848460197449 | 0.5950848460197449 | 0.5950848460197449 |\n",
       "| jensen_shannon_divergence | 1.4333720207214355 | 1.4333720207214355 | 1.4333720207214355 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-regard",
   "metadata": {},
   "source": [
    "### 0.01, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "practical-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5620816946029663 | 0.5620816946029663 | 0.5620816946029663 |\n",
       "| pearson_corr | 0.5154631733894348 | 0.5154631733894348 | 0.5154631733894348 |\n",
       "| jensen_shannon_divergence | 1.4506980180740356 | 1.4506980180740356 | 1.4506980180740356 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6314632892608643 | 0.6314632892608643 | 0.6314632892608643 |\n",
       "| pearson_corr | 0.49053487181663513 | 0.49053487181663513 | 0.49053487181663513 |\n",
       "| jensen_shannon_divergence | 1.8637430667877197 | 1.8637430667877197 | 1.8637430667877197 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.74514240026474 | 0.74514240026474 | 0.74514240026474 |\n",
       "| pearson_corr | 0.34005752205848694 | 0.34005752205848694 | 0.34005752205848694 |\n",
       "| jensen_shannon_divergence | 3.026334047317505 | 3.026334047317505 | 3.026334047317505 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5948427319526672 | 0.5948427319526672 | 0.5948427319526672 |\n",
       "| pearson_corr | 0.47788140177726746 | 0.47788140177726746 | 0.47788140177726746 |\n",
       "| jensen_shannon_divergence | 1.748281478881836 | 1.748281478881836 | 1.748281478881836 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49304816126823425 | 0.49304816126823425 | 0.49304816126823425 |\n",
       "| pearson_corr | 0.6153791546821594 | 0.6153791546821594 | 0.6153791546821594 |\n",
       "| jensen_shannon_divergence | 1.3658863306045532 | 1.3658863306045532 | 1.3658863306045532 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-express",
   "metadata": {},
   "source": [
    "### 0.01, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "purple-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5712888836860657 | 0.5712888836860657 | 0.5712888836860657 |\n",
       "| pearson_corr | 0.4438229501247406 | 0.4438229501247406 | 0.4438229501247406 |\n",
       "| jensen_shannon_divergence | 1.442650318145752 | 1.442650318145752 | 1.442650318145752 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49387380480766296 | 0.49387380480766296 | 0.49387380480766296 |\n",
       "| pearson_corr | 0.5869601964950562 | 0.5869601964950562 | 0.5869601964950562 |\n",
       "| jensen_shannon_divergence | 1.3759129047393799 | 1.3759129047393799 | 1.3759129047393799 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.664072573184967 | 0.664072573184967 | 0.664072573184967 |\n",
       "| pearson_corr | 0.3785347640514374 | 0.3785347640514374 | 0.3785347640514374 |\n",
       "| jensen_shannon_divergence | 1.937619686126709 | 1.937619686126709 | 1.937619686126709 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5857968926429749 | 0.5857968926429749 | 0.5857968926429749 |\n",
       "| pearson_corr | 0.5107135772705078 | 0.5107135772705078 | 0.5107135772705078 |\n",
       "| jensen_shannon_divergence | 1.7240110635757446 | 1.7240110635757446 | 1.7240110635757446 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4817771017551422 | 0.4817771017551422 | 0.4817771017551422 |\n",
       "| pearson_corr | 0.6184218525886536 | 0.6184218525886536 | 0.6184218525886536 |\n",
       "| jensen_shannon_divergence | 1.3420637845993042 | 1.3420637845993042 | 1.3420637845993042 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-cabin",
   "metadata": {},
   "source": [
    "### 0.01, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "gentle-australia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7534201741218567 | 0.7534201741218567 | 0.7534201741218567 |\n",
       "| pearson_corr | 0.39772671461105347 | 0.39772671461105347 | 0.39772671461105347 |\n",
       "| jensen_shannon_divergence | 2.7222695350646973 | 2.7222695350646973 | 2.7222695350646973 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47050318121910095 | 0.47050318121910095 | 0.47050318121910095 |\n",
       "| pearson_corr | 0.6201063394546509 | 0.6201063394546509 | 0.6201063394546509 |\n",
       "| jensen_shannon_divergence | 1.3275595903396606 | 1.3275595903396606 | 1.3275595903396606 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7657183408737183 | 0.7657183408737183 | 0.7657183408737183 |\n",
       "| pearson_corr | 0.3918047249317169 | 0.3918047249317169 | 0.3918047249317169 |\n",
       "| jensen_shannon_divergence | 2.8572304248809814 | 2.8572304248809814 | 2.8572304248809814 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6472703218460083 | 0.6472703218460083 | 0.6472703218460083 |\n",
       "| pearson_corr | 0.3543688952922821 | 0.3543688952922821 | 0.3543688952922821 |\n",
       "| jensen_shannon_divergence | 2.0513923168182373 | 2.0513923168182373 | 2.0513923168182373 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5292101502418518 | 0.5292101502418518 | 0.5292101502418518 |\n",
       "| pearson_corr | 0.5924198627471924 | 0.5924198627471924 | 0.5924198627471924 |\n",
       "| jensen_shannon_divergence | 1.5824764966964722 | 1.5824764966964722 | 1.5824764966964722 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
